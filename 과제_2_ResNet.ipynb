{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "과제 2 ResNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunW3/kmooc_spark/blob/master/%EA%B3%BC%EC%A0%9C_2_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YWcjwWOJbIu",
        "colab_type": "text"
      },
      "source": [
        "Copyright (C) 2018 Software Platform Lab, Seoul National University\n",
        "\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv1ZcltrbkFI",
        "colab_type": "text"
      },
      "source": [
        "# KMOOC_HW2: Training a ResNet model\n",
        "\n",
        "- Create input pipeline using Tensorflow Dataset API\n",
        "- Define fully connected layer\n",
        "- Define variable update operation using optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu9cVjyfbkz7",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "256f6d81-5617-4123-841b-b8f745eb54a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#@title Run me to download the CIFAR-10 dataset!\n",
        "# https://blog.shichao.io/2012/10/04/progress_speed_indicator_for_urlretrieve_in_python.html\n",
        "\n",
        "import os, sys, time\n",
        "import tarfile\n",
        "import urllib\n",
        "\n",
        "def reporthook(count, block_size, total_size):\n",
        "  global start_time\n",
        "  if count == 0:\n",
        "    start_time = time.time()\n",
        "    return\n",
        "  duration = time.time() - start_time\n",
        "  progress_size = int(count * block_size)\n",
        "  percent = int(count * block_size * 100 / total_size)\n",
        "  sys.stdout.write('\\r...%d%%, %d MB, %d seconds passed' %\n",
        "                   (percent, progress_size / (1024 * 1024), duration))\n",
        "  sys.stdout.flush()\n",
        "\n",
        "cifar10url = 'https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
        "cifar10 = cifar10url.split('/')[-1]\n",
        "\n",
        "if not os.path.isfile(cifar10):\n",
        "  urllib.urlretrieve(cifar10url, cifar10, reporthook)\n",
        "print()\n",
        "print('Download finished!')\n",
        "\n",
        "cifar10_extracted = 'cifar-10-batches-bin'\n",
        "\n",
        "if not os.path.isdir(cifar10_extracted):\n",
        "  tarfile.open(cifar10, 'r:gz').extractall()\n",
        "print('Uncompression finished!')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "()\n",
            "Download finished!\n",
            "Uncompression finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o4xeczHddip",
        "colab_type": "code",
        "outputId": "49415740-7e2a-461a-a9e5-460a8e2da87e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir train_ckpt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘train_ckpt’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBGRBH9fg742",
        "colab_type": "code",
        "outputId": "62b1e132-1d08-45c0-8d06-fcce417fe99e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cifar-10-batches-bin\tngrok\t\t\t      sample_data  train_ckpt\n",
            "cifar-10-binary.tar.gz\tngrok-stable-linux-amd64.zip  tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc0UJQMReBnw",
        "colab_type": "text"
      },
      "source": [
        "## Problem 1. CIFAR10 input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUdHzsItMc4d",
        "colab_type": "code",
        "outputId": "bffc1d7d-3cb4-49e7-8712-d45f095564af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"CIFAR dataset input module.\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def build_input(dataset, data_path, batch_size, mode):\n",
        "  \"\"\"Build CIFAR image and labels.\n",
        "\n",
        "  Args:\n",
        "    dataset: Either 'cifar10' or 'cifar100'.\n",
        "    data_path: Filename for data.\n",
        "    batch_size: Input batch size.\n",
        "    mode: Either 'train' or 'eval'.\n",
        "  Returns:\n",
        "    images: Batches of images. [batch_size, image_size, image_size, 3]\n",
        "    labels: Batches of labels. [batch_size, num_classes]\n",
        "  Raises:\n",
        "    ValueError: when the specified dataset is not supported.\n",
        "  \"\"\"\n",
        "  image_size = 32\n",
        "  if dataset == 'cifar10':\n",
        "    label_bytes = 1\n",
        "    label_offset = 0\n",
        "    num_classes = 10\n",
        "  else:\n",
        "    raise ValueError('Not supported dataset %s', dataset)\n",
        "\n",
        "  depth = 3\n",
        "  image_bytes = image_size * image_size * depth\n",
        "  record_bytes = label_bytes + label_offset + image_bytes\n",
        "\n",
        "  def parse_data(value): \n",
        "    # Convert these examples to dense labels and processed images.\n",
        "    record = tf.reshape(tf.decode_raw(value, tf.uint8), [record_bytes])\n",
        "    label = tf.cast(tf.slice(record, [label_offset], [label_bytes]), tf.int32)\n",
        "\n",
        "    # Convert from string to [depth * height * width] to [depth, height, width].\n",
        "    depth_major = tf.reshape(tf.slice(record, [label_offset + label_bytes], [image_bytes]),\n",
        "                           [depth, image_size, image_size])\n",
        "    # Convert from [depth, height, width] to [height, width, depth].\n",
        "    image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n",
        "\n",
        "    if mode == 'train':\n",
        "      image = tf.image.resize_image_with_crop_or_pad(\n",
        "        image, image_size+4, image_size+4)\n",
        "      image = tf.random_crop(image, [image_size, image_size, 3])\n",
        "      image = tf.image.random_flip_left_right(image)\n",
        "      image = tf.image.per_image_standardization(image)\n",
        "\n",
        "    else:\n",
        "      image = tf.image.resize_image_with_crop_or_pad(\n",
        "        image, image_size, image_size)\n",
        "      image = tf.image.per_image_standardization(image)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "  data_files = tf.gfile.Glob(data_path)\n",
        "  # return : A list of strings containing filenames that match the given pattern(s).\n",
        "  data_files.sort()\n",
        "  #############################################################################\n",
        "  #### FIXME: Create an input pipline using tf.data.Dataset and parse_data ####\n",
        "  \n",
        "  data_files = tf.constant(data_files)\n",
        "  data_files = tf.data.Dataset.from_tensor_slices(data_files)\n",
        "  ds = tf.data.FixedLengthRecordDataset(data_files,record_bytes)\n",
        "  ds = ds.map(parse_data).shuffle(100000).repeat().batch(batch_size)\n",
        "  #############################################################################\n",
        "\n",
        "\n",
        "  \n",
        "  iterator = ds.make_one_shot_iterator()\n",
        "  images, labels = iterator.get_next()\n",
        "  \n",
        "  #original\n",
        "  assert images.shape[1] ==  images.shape[2] == image_size\n",
        "  assert images.shape[3] == depth\n",
        "  \n",
        "  #alternative\n",
        "  #assert images.shape[0] == images.shape[1] == image_size\n",
        "  #assert images.shape[2] == depth\n",
        "  \n",
        "  assert labels.shape[1] == 1\n",
        "  \n",
        "  images = tf.reshape(images, [batch_size, image_size, image_size, depth])\n",
        "  labels = tf.reshape(labels, [batch_size,  +1])\n",
        "  indices = tf.reshape(tf.range(0, batch_size, 1), [batch_size, 1])\n",
        "  labels = tf.sparse_to_dense(\n",
        "      tf.concat(values=[indices, labels], axis=1),\n",
        "      [batch_size, num_classes], 1.0, 0.0) \n",
        "  return images, labels\n",
        "\n",
        "train_data_path = './cifar-10-batches-bin/data_batch*'\n",
        "batch_size = 128\n",
        "with tf.Graph().as_default():\n",
        "    images, labels = build_input(\n",
        "        'cifar10', train_data_path, batch_size, 'train')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0630 00:49:21.207746 140438030301056 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0630 00:49:21.225564 140438030301056 deprecation.py:323] From <ipython-input-4-c4d1d5ea4d4c>:72: make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
            "W0630 00:49:21.246948 140438030301056 deprecation.py:323] From <ipython-input-4-c4d1d5ea4d4c>:90: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ613UElFNNn",
        "colab_type": "text"
      },
      "source": [
        "   \n",
        "     \n",
        "\n",
        "data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
        "\n",
        "labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47vs_zxwbzgS",
        "colab_type": "text"
      },
      "source": [
        "##Problem 2. Define FC layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAwLxH90Ormy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fully_connected(batch_size, x, out_dim):\n",
        "    \"\"\"FullyConnected layer for final output.\"\"\"\n",
        "    x = tf.reshape(x, [batch_size, -1])\n",
        "    w = tf.get_variable(\n",
        "        'DW', [x.get_shape()[1], out_dim],\n",
        "        initializer=tf.uniform_unit_scaling_initializer(factor=1.0))\n",
        "    \n",
        "    ################################################\n",
        "    #### FIXME: Create an variable 'b'          ####\n",
        "    #### HINT: name: 'biases', shape: [out_dim] ####\n",
        "    ####       use constant_initializer         ####\n",
        "    ################################################\n",
        "    b = tf.get_variable(\n",
        "      'biases', shape=[out_dim], initializer= tf.constant_initializer(0.0))\n",
        "    \n",
        "    ##################################################\n",
        "    #### FIXME: Create an xw_plus_b op            ####\n",
        "    #### HINT: xw+b (xw is matrix multiplication) ####\n",
        "    ##################################################\n",
        "  \n",
        "    xw_plus_b = tf.matmul(x,w)+b\n",
        "    return xw_plus_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7bOoR5QUTkE",
        "colab_type": "text"
      },
      "source": [
        "## Problem 3. Define optimizer and update operation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN9vleA0UXow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_train_op(loss, lrn_rate):\n",
        "    \"\"\"Build training specific ops for the graph.\"\"\"\n",
        "\n",
        "    #########################################################################\n",
        "    #### FIXME: Create an optimizer using self.lrn_rate as learning rate ####\n",
        "    #########################################################################\n",
        "    update_op = tf.train.GradientDescentOptimizer(learning_rate=lrn_rate).minimize(loss)\n",
        "    \n",
        "    return update_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1UUs2SrWwye",
        "colab_type": "text"
      },
      "source": [
        "## Define ResNet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL5o4WI7b2-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"ResNet model.\n",
        "\n",
        "Related papers:\n",
        "https://arxiv.org/pdf/1603.05027v2.pdf\n",
        "https://arxiv.org/pdf/1512.03385v1.pdf\n",
        "https://arxiv.org/pdf/1605.07146v1.pdf\n",
        "\"\"\"\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import six\n",
        "\n",
        "from tensorflow.python.training import moving_averages\n",
        "\n",
        "\n",
        "HParams = namedtuple('HParams',\n",
        "                     'batch_size, num_classes, min_lrn_rate, lrn_rate, '\n",
        "                     'num_residual_units, use_bottleneck, weight_decay_rate, '\n",
        "                     'relu_leakiness')\n",
        "\n",
        "\n",
        "class ResNet(object):\n",
        "  \"\"\"ResNet model.\"\"\"\n",
        "\n",
        "  def __init__(self, hps, images, labels, mode):\n",
        "    \"\"\"ResNet constructor.\n",
        "\n",
        "    Args:\n",
        "      hps: Hyperparameters.\n",
        "      images: Batches of images. [batch_size, image_size, image_size, 3]\n",
        "      labels: Batches of labels. [batch_size, num_classes]\n",
        "      mode: One of 'train' and 'eval'.\n",
        "    \"\"\"\n",
        "    self.hps = hps\n",
        "    self._images = images\n",
        "    self.labels = labels\n",
        "    self.mode = mode\n",
        "\n",
        "    self._extra_train_ops = []\n",
        "\n",
        "  def build_graph(self):\n",
        "    \"\"\"Build a whole graph for the model.\"\"\"\n",
        "    self.global_step = tf.train.get_or_create_global_step()\n",
        "    self._build_model()\n",
        "    if self.mode == 'train':\n",
        "      self.lrn_rate =  tf.constant(self.hps.lrn_rate, tf.float32)\n",
        "      update_op = build_train_op(self.cost, self.lrn_rate)\n",
        "      \n",
        "      with tf.control_dependencies([update_op]):\n",
        "          apply_op = tf.assign_add(self.global_step, 1)\n",
        "   \n",
        "      train_ops = [apply_op] + self._extra_train_ops\n",
        "      self.train_op = tf.group(*train_ops)\n",
        "      \n",
        "    self.summaries = tf.summary.merge_all()\n",
        "\n",
        "  def _stride_arr(self, stride):\n",
        "    \"\"\"Map a stride scalar to the stride array for tf.nn.conv2d.\"\"\"\n",
        "    return [1, stride, stride, 1]\n",
        "\n",
        "  def _build_model(self):\n",
        "    \"\"\"Build the core model within the graph.\"\"\"\n",
        "    with tf.variable_scope('init'):\n",
        "      x = self._images\n",
        "      x = self._conv('init_conv', x, 3, 3, 16, self._stride_arr(1))\n",
        "\n",
        "    strides = [1, 2, 2]\n",
        "    activate_before_residual = [True, False, False]\n",
        "    if self.hps.use_bottleneck:\n",
        "      res_func = self._bottleneck_residual\n",
        "      filters = [16, 64, 128, 256]\n",
        "    else:\n",
        "      res_func = self._residual\n",
        "      filters = [16, 16, 32, 64]\n",
        "      # Uncomment the following codes to use w28-10 wide residual network.\n",
        "      # It is more memory efficient than very deep residual network and has\n",
        "      # comparably good performance.\n",
        "      # https://arxiv.org/pdf/1605.07146v1.pdf\n",
        "      # filters = [16, 160, 320, 640]\n",
        "      # Update hps.num_residual_units to 4\n",
        "\n",
        "    with tf.variable_scope('unit_1_0'):\n",
        "      x = res_func(x, filters[0], filters[1], self._stride_arr(strides[0]),\n",
        "                   activate_before_residual[0])\n",
        "    for i in six.moves.range(1, self.hps.num_residual_units):\n",
        "      with tf.variable_scope('unit_1_%d' % i):\n",
        "        x = res_func(x, filters[1], filters[1], self._stride_arr(1), False)\n",
        "\n",
        "    with tf.variable_scope('unit_2_0'):\n",
        "      x = res_func(x, filters[1], filters[2], self._stride_arr(strides[1]),\n",
        "                   activate_before_residual[1])\n",
        "    for i in six.moves.range(1, self.hps.num_residual_units):\n",
        "      with tf.variable_scope('unit_2_%d' % i):\n",
        "        x = res_func(x, filters[2], filters[2], self._stride_arr(1), False)\n",
        "\n",
        "    with tf.variable_scope('unit_3_0'):\n",
        "      x = res_func(x, filters[2], filters[3], self._stride_arr(strides[2]),\n",
        "                   activate_before_residual[2])\n",
        "    for i in six.moves.range(1, self.hps.num_residual_units):\n",
        "      with tf.variable_scope('unit_3_%d' % i):\n",
        "        x = res_func(x, filters[3], filters[3], self._stride_arr(1), False)\n",
        "\n",
        "    with tf.variable_scope('unit_last'):\n",
        "      x = self._batch_norm('final_bn', x)\n",
        "      x = self._relu(x, self.hps.relu_leakiness)\n",
        "      x = self._global_avg_pool(x)\n",
        "\n",
        "    with tf.variable_scope('logit'):\n",
        "      logits = fully_connected(self.hps.batch_size, x, self.hps.num_classes)\n",
        "      self.predictions = tf.nn.softmax(logits)\n",
        "\n",
        "    with tf.variable_scope('costs'):\n",
        "      xent = tf.nn.softmax_cross_entropy_with_logits(\n",
        "          logits=logits, labels=self.labels)\n",
        "      self.cost = tf.reduce_mean(xent, name='xent')\n",
        "      self.cost += self._decay()\n",
        "\n",
        "      tf.summary.scalar('cost', self.cost)\n",
        "\n",
        "  # TODO(xpan): Consider batch_norm in contrib/layers/python/layers/layers.py\n",
        "  def _batch_norm(self, name, x):\n",
        "    \"\"\"Batch normalization.\"\"\"\n",
        "    with tf.variable_scope(name):\n",
        "      params_shape = [x.get_shape()[-1]]\n",
        "\n",
        "      beta = tf.get_variable(\n",
        "          'beta', params_shape, tf.float32,\n",
        "          initializer=tf.constant_initializer(0.0, tf.float32))\n",
        "      gamma = tf.get_variable(\n",
        "          'gamma', params_shape, tf.float32,\n",
        "          initializer=tf.constant_initializer(1.0, tf.float32))\n",
        "\n",
        "      if self.mode == 'train':\n",
        "        mean, variance = tf.nn.moments(x, [0, 1, 2], name='moments')\n",
        "\n",
        "        moving_mean = tf.get_variable(\n",
        "            'moving_mean', params_shape, tf.float32,\n",
        "            initializer=tf.constant_initializer(0.0, tf.float32),\n",
        "            trainable=False)\n",
        "        moving_variance = tf.get_variable(\n",
        "            'moving_variance', params_shape, tf.float32,\n",
        "            initializer=tf.constant_initializer(1.0, tf.float32),\n",
        "            trainable=False)\n",
        "\n",
        "        self._extra_train_ops.append(moving_averages.assign_moving_average(\n",
        "            moving_mean, mean, 0.9))\n",
        "        self._extra_train_ops.append(moving_averages.assign_moving_average(\n",
        "            moving_variance, variance, 0.9))\n",
        "      else:\n",
        "        mean = tf.get_variable(\n",
        "            'moving_mean', params_shape, tf.float32,\n",
        "            initializer=tf.constant_initializer(0.0, tf.float32),\n",
        "            trainable=False)\n",
        "        variance = tf.get_variable(\n",
        "            'moving_variance', params_shape, tf.float32,\n",
        "            initializer=tf.constant_initializer(1.0, tf.float32),\n",
        "            trainable=False)\n",
        "        tf.summary.histogram(mean.op.name, mean)\n",
        "        tf.summary.histogram(variance.op.name, variance)\n",
        "      # epsilon used to be 1e-5. Maybe 0.001 solves NaN problem in deeper net.\n",
        "      y = tf.nn.batch_normalization(\n",
        "          x, mean, variance, beta, gamma, 0.001)\n",
        "      y.set_shape(x.get_shape())\n",
        "      return y\n",
        "\n",
        "  def _residual(self, x, in_filter, out_filter, stride,\n",
        "                activate_before_residual=False):\n",
        "    \"\"\"Residual unit with 2 sub layers.\"\"\"\n",
        "    if activate_before_residual:\n",
        "      with tf.variable_scope('shared_activation'):\n",
        "        x = self._batch_norm('init_bn', x)\n",
        "        x = self._relu(x, self.hps.relu_leakiness)\n",
        "        orig_x = x\n",
        "    else:\n",
        "      with tf.variable_scope('residual_only_activation'):\n",
        "        orig_x = x\n",
        "        x = self._batch_norm('init_bn', x)\n",
        "        x = self._relu(x, self.hps.relu_leakiness)\n",
        "\n",
        "    with tf.variable_scope('sub1'):\n",
        "      x = self._conv('conv1', x, 3, in_filter, out_filter, stride)\n",
        "\n",
        "    with tf.variable_scope('sub2'):\n",
        "      x = self._batch_norm('bn2', x)\n",
        "      x = self._relu(x, self.hps.relu_leakiness)\n",
        "      x = self._conv('conv2', x, 3, out_filter, out_filter, [1, 1, 1, 1])\n",
        "\n",
        "    with tf.variable_scope('sub_add'):\n",
        "      if in_filter != out_filter:\n",
        "        orig_x = tf.nn.avg_pool(orig_x, stride, stride, 'VALID')\n",
        "        orig_x = tf.pad(\n",
        "            orig_x, [[0, 0], [0, 0], [0, 0],\n",
        "                     [(out_filter-in_filter)//2, (out_filter-in_filter)//2]])\n",
        "      x += orig_x\n",
        "\n",
        "    tf.logging.debug('image after unit %s', x.get_shape())\n",
        "    return x\n",
        "\n",
        "  def _bottleneck_residual(self, x, in_filter, out_filter, stride,\n",
        "                           activate_before_residual=False):\n",
        "    \"\"\"Bottleneck residual unit with 3 sub layers.\"\"\"\n",
        "    if activate_before_residual:\n",
        "      with tf.variable_scope('common_bn_relu'):\n",
        "        x = self._batch_norm('init_bn', x)\n",
        "        x = self._relu(x, self.hps.relu_leakiness)\n",
        "        orig_x = x\n",
        "    else:\n",
        "      with tf.variable_scope('residual_bn_relu'):\n",
        "        orig_x = x\n",
        "        x = self._batch_norm('init_bn', x)\n",
        "        x = self._relu(x, self.hps.relu_leakiness)\n",
        "\n",
        "    with tf.variable_scope('sub1'):\n",
        "      x = self._conv('conv1', x, 1, in_filter, out_filter/4, stride)\n",
        "\n",
        "    with tf.variable_scope('sub2'):\n",
        "      x = self._batch_norm('bn2', x)\n",
        "      x = self._relu(x, self.hps.relu_leakiness)\n",
        "      x = self._conv('conv2', x, 3, out_filter/4, out_filter/4, [1, 1, 1, 1])\n",
        "\n",
        "    with tf.variable_scope('sub3'):\n",
        "      x = self._batch_norm('bn3', x)\n",
        "      x = self._relu(x, self.hps.relu_leakiness)\n",
        "      x = self._conv('conv3', x, 1, out_filter/4, out_filter, [1, 1, 1, 1])\n",
        "\n",
        "    with tf.variable_scope('sub_add'):\n",
        "      if in_filter != out_filter:\n",
        "        orig_x = self._conv('project', orig_x, 1, in_filter, out_filter, stride)\n",
        "      x += orig_x\n",
        "\n",
        "    tf.logging.info('image after unit %s', x.get_shape())\n",
        "    return x\n",
        "\n",
        "  def _decay(self):\n",
        "    \"\"\"L2 weight decay loss.\"\"\"\n",
        "    costs = []\n",
        "    for var in tf.trainable_variables():\n",
        "      if var.op.name.find(r'DW') > 0:\n",
        "        costs.append(tf.nn.l2_loss(var))\n",
        "        # tf.summary.histogram(var.op.name, var)\n",
        "\n",
        "    return tf.multiply(self.hps.weight_decay_rate, tf.add_n(costs))\n",
        "\n",
        "  def _conv(self, name, x, filter_size, in_filters, out_filters, strides):\n",
        "    \"\"\"Convolution.\"\"\"\n",
        "    with tf.variable_scope(name):\n",
        "      n = filter_size * filter_size * out_filters\n",
        "      kernel = tf.get_variable(\n",
        "          'DW', [filter_size, filter_size, in_filters, out_filters],\n",
        "          tf.float32, initializer=tf.random_normal_initializer(\n",
        "              stddev=np.sqrt(2.0/n)))\n",
        "      return tf.nn.conv2d(x, kernel, strides, padding='SAME')\n",
        "\n",
        "  def _relu(self, x, leakiness=0.0):\n",
        "    \"\"\"Relu, with optional leaky support.\"\"\"\n",
        "    return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')\n",
        "\n",
        "  def _global_avg_pool(self, x):\n",
        "    assert x.get_shape().ndims == 4\n",
        "    return tf.reduce_mean(x, [1, 2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYszRyxjb2aZ",
        "colab_type": "text"
      },
      "source": [
        "## Train ResNet model\n",
        "Do not be frightened if you face such an error: \n",
        "`An exception has occurred, use %tb to see the full traceback.  SystemExit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR7jlpOUdQhl",
        "colab_type": "code",
        "outputId": "56476db2-d5d6-4c48-e639-3952c4d52e88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"ResNet Train/Eval module.\n",
        "\"\"\"\n",
        "import time\n",
        "import six\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Global\n",
        "train_data_path = './cifar-10-batches-bin/data_batch*'\n",
        "image_size = 32\n",
        "ckpt_dir = './train_ckpt'\n",
        "ckpt_prefix = ckpt_dir + '/cifar10-train'\n",
        "\n",
        "def train(hps):\n",
        "  \"\"\"Training loop.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    images, labels = build_input(\n",
        "    'cifar10', train_data_path, hps.batch_size, 'train')\n",
        "    model = ResNet(hps, images, labels, 'train')\n",
        "    model.build_graph()\n",
        "\n",
        "    truth = tf.argmax(model.labels, axis=1)\n",
        "    predictions = tf.argmax(model.predictions, axis=1)\n",
        "    precision = tf.reduce_mean(tf.to_float(tf.equal(predictions, truth)))\n",
        "  \n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    saver = tf.train.Saver(max_to_keep=10000)\n",
        "  \n",
        "    with tf.Session() as sess:\n",
        "      sess.run(init)\n",
        "      for i in range(3001):\n",
        "        _, global_step, cost, precision_ = \\\n",
        "          sess.run([model.train_op, model.global_step, model.cost, precision])\n",
        "    \n",
        "        if global_step % 100 == 0:\n",
        "          print('step: %d, loss: %.3f, precision: %.3f' % (global_step, cost, precision_))\n",
        "\n",
        "          saver.save(sess, ckpt_prefix, global_step=i)\n",
        "\n",
        "        \n",
        "def main(_):\n",
        "  batch_size = 128\n",
        "\n",
        "  hps = HParams(batch_size=batch_size,\n",
        "                             num_classes=10,\n",
        "                             min_lrn_rate=0.0001,\n",
        "                             lrn_rate=0.1,\n",
        "                             num_residual_units=5,\n",
        "                             use_bottleneck=False,\n",
        "                             weight_decay_rate=0.0002,\n",
        "                             relu_leakiness=0.1)\n",
        "\n",
        "  train(hps)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.run()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0630 00:49:21.838732 140438030301056 deprecation.py:506] From <ipython-input-7-c2c204c5fa76>:130: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0630 00:49:21.890649 140438030301056 deprecation.py:323] From <ipython-input-7-c2c204c5fa76>:258: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0630 00:49:23.847703 140438030301056 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py:507: __init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
            "W0630 00:49:23.862502 140438030301056 deprecation.py:323] From <ipython-input-7-c2c204c5fa76>:116: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "W0630 00:49:26.850739 140438030301056 deprecation.py:323] From <ipython-input-8-74d36d64d250>:27: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step: 0, loss: 2.598, precision: 0.086\n",
            "step: 100, loss: 1.985, precision: 0.414\n",
            "step: 200, loss: 1.777, precision: 0.414\n",
            "step: 300, loss: 1.708, precision: 0.477\n",
            "step: 400, loss: 1.507, precision: 0.516\n",
            "step: 500, loss: 1.366, precision: 0.547\n",
            "step: 600, loss: 1.322, precision: 0.594\n",
            "step: 700, loss: 1.274, precision: 0.617\n",
            "step: 800, loss: 1.152, precision: 0.680\n",
            "step: 900, loss: 1.207, precision: 0.648\n",
            "step: 1000, loss: 1.293, precision: 0.625\n",
            "step: 1100, loss: 1.105, precision: 0.648\n",
            "step: 1200, loss: 1.019, precision: 0.711\n",
            "step: 1300, loss: 1.046, precision: 0.766\n",
            "step: 1400, loss: 1.001, precision: 0.719\n",
            "step: 1500, loss: 1.114, precision: 0.625\n",
            "step: 1600, loss: 1.284, precision: 0.609\n",
            "step: 1700, loss: 0.906, precision: 0.758\n",
            "step: 1800, loss: 0.883, precision: 0.727\n",
            "step: 1900, loss: 0.975, precision: 0.742\n",
            "step: 2000, loss: 1.056, precision: 0.719\n",
            "step: 2100, loss: 1.050, precision: 0.703\n",
            "step: 2200, loss: 0.905, precision: 0.727\n",
            "step: 2300, loss: 0.875, precision: 0.758\n",
            "step: 2400, loss: 0.802, precision: 0.789\n",
            "step: 2500, loss: 0.867, precision: 0.750\n",
            "step: 2600, loss: 0.986, precision: 0.695\n",
            "step: 2700, loss: 0.947, precision: 0.766\n",
            "step: 2800, loss: 0.798, precision: 0.820\n",
            "step: 2900, loss: 0.969, precision: 0.758\n",
            "step: 3000, loss: 0.871, precision: 0.766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y3HiS9MtcxW",
        "colab_type": "text"
      },
      "source": [
        "### You can see *cifar10-train-0~3000* checkpoint files when you run following code, after you train model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg6E42w4oTB1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "outputId": "6e9c9fb0-1062-4d89-e651-47caad392863"
      },
      "source": [
        "!ls train_ckpt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\t\t\t\tcifar10-train-2200.index\n",
            "cifar10-train-0.data-00000-of-00001\tcifar10-train-2200.meta\n",
            "cifar10-train-0.index\t\t\tcifar10-train-2300.data-00000-of-00001\n",
            "cifar10-train-0.meta\t\t\tcifar10-train-2300.index\n",
            "cifar10-train-1000.data-00000-of-00001\tcifar10-train-2300.meta\n",
            "cifar10-train-1000.index\t\tcifar10-train-2400.data-00000-of-00001\n",
            "cifar10-train-1000.meta\t\t\tcifar10-train-2400.index\n",
            "cifar10-train-100.data-00000-of-00001\tcifar10-train-2400.meta\n",
            "cifar10-train-100.index\t\t\tcifar10-train-2500.data-00000-of-00001\n",
            "cifar10-train-100.meta\t\t\tcifar10-train-2500.index\n",
            "cifar10-train-1100.data-00000-of-00001\tcifar10-train-2500.meta\n",
            "cifar10-train-1100.index\t\tcifar10-train-2600.data-00000-of-00001\n",
            "cifar10-train-1100.meta\t\t\tcifar10-train-2600.index\n",
            "cifar10-train-1200.data-00000-of-00001\tcifar10-train-2600.meta\n",
            "cifar10-train-1200.index\t\tcifar10-train-2700.data-00000-of-00001\n",
            "cifar10-train-1200.meta\t\t\tcifar10-train-2700.index\n",
            "cifar10-train-1300.data-00000-of-00001\tcifar10-train-2700.meta\n",
            "cifar10-train-1300.index\t\tcifar10-train-2800.data-00000-of-00001\n",
            "cifar10-train-1300.meta\t\t\tcifar10-train-2800.index\n",
            "cifar10-train-1400.data-00000-of-00001\tcifar10-train-2800.meta\n",
            "cifar10-train-1400.index\t\tcifar10-train-2900.data-00000-of-00001\n",
            "cifar10-train-1400.meta\t\t\tcifar10-train-2900.index\n",
            "cifar10-train-1500.data-00000-of-00001\tcifar10-train-2900.meta\n",
            "cifar10-train-1500.index\t\tcifar10-train-3000.data-00000-of-00001\n",
            "cifar10-train-1500.meta\t\t\tcifar10-train-3000.index\n",
            "cifar10-train-1600.data-00000-of-00001\tcifar10-train-3000.meta\n",
            "cifar10-train-1600.index\t\tcifar10-train-300.data-00000-of-00001\n",
            "cifar10-train-1600.meta\t\t\tcifar10-train-300.index\n",
            "cifar10-train-1700.data-00000-of-00001\tcifar10-train-300.meta\n",
            "cifar10-train-1700.index\t\tcifar10-train-400.data-00000-of-00001\n",
            "cifar10-train-1700.meta\t\t\tcifar10-train-400.index\n",
            "cifar10-train-1800.data-00000-of-00001\tcifar10-train-400.meta\n",
            "cifar10-train-1800.index\t\tcifar10-train-500.data-00000-of-00001\n",
            "cifar10-train-1800.meta\t\t\tcifar10-train-500.index\n",
            "cifar10-train-1900.data-00000-of-00001\tcifar10-train-500.meta\n",
            "cifar10-train-1900.index\t\tcifar10-train-600.data-00000-of-00001\n",
            "cifar10-train-1900.meta\t\t\tcifar10-train-600.index\n",
            "cifar10-train-2000.data-00000-of-00001\tcifar10-train-600.meta\n",
            "cifar10-train-2000.index\t\tcifar10-train-700.data-00000-of-00001\n",
            "cifar10-train-2000.meta\t\t\tcifar10-train-700.index\n",
            "cifar10-train-200.data-00000-of-00001\tcifar10-train-700.meta\n",
            "cifar10-train-200.index\t\t\tcifar10-train-800.data-00000-of-00001\n",
            "cifar10-train-200.meta\t\t\tcifar10-train-800.index\n",
            "cifar10-train-2100.data-00000-of-00001\tcifar10-train-800.meta\n",
            "cifar10-train-2100.index\t\tcifar10-train-900.data-00000-of-00001\n",
            "cifar10-train-2100.meta\t\t\tcifar10-train-900.index\n",
            "cifar10-train-2200.data-00000-of-00001\tcifar10-train-900.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBDxI6zhoyY_",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate trained ResNet model\n",
        "\n",
        "Before you run this code\n",
        "click Runtime->**restart runtime**\n",
        "\n",
        "(If you want to erase all the local files, then click *RESET ALL RUNTIMES* or **DO NOT CLICK!**)\n",
        "\n",
        "and restart **Define the Resnet50 Model**,  **CIFAR10 input**\n",
        "\n",
        "Do not be frightened if you face such an error: \n",
        "`An exception has occurred, use %tb to see the full traceback.  SystemExit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj2qiublo3hU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a79025e8-d58f-4e98-c21e-6b0c9c5131e3"
      },
      "source": [
        "!rm -rf './tensorboard'\n",
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"ResNet Train/Eval module.\n",
        "\"\"\"\n",
        "import time\n",
        "import six\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "eval_data_path = './cifar-10-batches-bin/test_batch.bin'\n",
        "ckpt_dir = './train_ckpt'\n",
        "tensorboard_path = './tensorboard'\n",
        "\n",
        "def evaluate(hps):\n",
        "  \"\"\"Eval loop.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    images, labels = build_input(\n",
        "      'cifar10', './cifar-10-batches-bin/test_batch.bin', hps.batch_size, 'eval')\n",
        "    model = ResNet(hps, images, labels, 'eval')\n",
        "    model.build_graph()\n",
        "\n",
        "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
        "  \n",
        "    saver = tf.train.Saver() \n",
        "    \n",
        "    summary_writer = tf.summary.FileWriter('./tensorboard', sess.graph)\n",
        "\n",
        "    try:\n",
        "      ckpt_state = tf.train.get_checkpoint_state(ckpt_dir)\n",
        "    except tf.errors.OutOfRangeError as e:\n",
        "      tf.logging.error('Cannot restore checkpoint: %s', e)\n",
        "    if not (ckpt_state):\n",
        "      tf.logging.info('No model to eval yet at %s', ckpt_dir)\n",
        "    \n",
        "    best_precision = 0.\n",
        "    for i in range(len(ckpt_state.all_model_checkpoint_paths)):\n",
        "      tf.logging.info('Loading checkpoint %s', ckpt_state.all_model_checkpoint_paths[i])\n",
        "      saver.restore(sess, ckpt_state.all_model_checkpoint_paths[i])\n",
        "      total_prediction, correct_prediction = 0, 0\n",
        "\n",
        "      for _ in six.moves.range(100):\n",
        "        (summaries, loss, predictions, truth, train_step) = sess.run(\n",
        "          [model.summaries, model.cost, model.predictions,\n",
        "           model.labels, model.global_step])\n",
        "\n",
        "        truth = np.argmax(truth, axis=1)\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        correct_prediction += np.sum(truth == predictions)\n",
        "        total_prediction += predictions.shape[0]\n",
        "\n",
        "      precision = 1.0 * correct_prediction / total_prediction\n",
        "      best_precision = max(precision, best_precision)\n",
        "    \n",
        "      precision_summ = tf.Summary()\n",
        "      precision_summ.value.add(\n",
        "        tag='Precision', simple_value=precision)\n",
        "      summary_writer.add_summary(precision_summ, train_step)\n",
        "      best_precision_summ = tf.Summary()\n",
        "      best_precision_summ.value.add(\n",
        "        tag='Best Precision', simple_value=best_precision)\n",
        "      summary_writer.add_summary(best_precision_summ, train_step)\n",
        "      summary_writer.add_summary(summaries, train_step)\n",
        "     \n",
        "      tf.logging.info('loss: %.3f, precision: %.3f, best precision: %.3f' %\n",
        "                      (loss, precision, best_precision))\n",
        "      summary_writer.flush()\n",
        "\n",
        "      tf.logging.info('step: %d, loss: %.3f, precision: %.3f' %\n",
        "                      (i * 100, loss, precision))\n",
        "\n",
        "\n",
        "def main(_):\n",
        "\n",
        "  hps = HParams(batch_size=100,\n",
        "                num_classes=10,\n",
        "                min_lrn_rate=0.0001,\n",
        "                lrn_rate=0.1,\n",
        "                num_residual_units=5,\n",
        "                use_bottleneck=False,\n",
        "                weight_decay_rate=0.0002,\n",
        "                relu_leakiness=0.1)\n",
        "\n",
        "  evaluate(hps)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.run()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0630 00:59:19.312237 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-0\n",
            "W0630 00:59:19.313702 140438030301056 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0630 00:59:19.324604 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-0\n",
            "I0630 00:59:27.352547 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 2.631, precision: 0.094, best precision: 0.094\n",
            "I0630 00:59:27.354285 140438030301056 <ipython-input-11-f314403f781c>:85] step: 0, loss: 2.631, precision: 0.094\n",
            "I0630 00:59:27.356601 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-100\n",
            "I0630 00:59:27.362704 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-100\n",
            "I0630 00:59:30.455712 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 2.095, precision: 0.347, best precision: 0.347\n",
            "I0630 00:59:30.457231 140438030301056 <ipython-input-11-f314403f781c>:85] step: 100, loss: 2.095, precision: 0.347\n",
            "I0630 00:59:30.458941 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-200\n",
            "I0630 00:59:30.467241 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-200\n",
            "I0630 00:59:33.544774 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.821, precision: 0.398, best precision: 0.398\n",
            "I0630 00:59:33.546885 140438030301056 <ipython-input-11-f314403f781c>:85] step: 200, loss: 1.821, precision: 0.398\n",
            "I0630 00:59:33.552160 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-300\n",
            "I0630 00:59:33.558293 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-300\n",
            "I0630 00:59:36.641181 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.604, precision: 0.464, best precision: 0.464\n",
            "I0630 00:59:36.643069 140438030301056 <ipython-input-11-f314403f781c>:85] step: 300, loss: 1.604, precision: 0.464\n",
            "I0630 00:59:36.648010 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-400\n",
            "I0630 00:59:36.655556 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-400\n",
            "I0630 00:59:39.852766 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.659, precision: 0.492, best precision: 0.492\n",
            "I0630 00:59:39.854511 140438030301056 <ipython-input-11-f314403f781c>:85] step: 400, loss: 1.659, precision: 0.492\n",
            "I0630 00:59:39.858144 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-500\n",
            "I0630 00:59:39.865267 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-500\n",
            "I0630 00:59:42.921880 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.730, precision: 0.493, best precision: 0.493\n",
            "I0630 00:59:42.923722 140438030301056 <ipython-input-11-f314403f781c>:85] step: 500, loss: 1.730, precision: 0.493\n",
            "I0630 00:59:42.925158 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-600\n",
            "I0630 00:59:42.932780 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-600\n",
            "I0630 00:59:46.001192 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.623, precision: 0.523, best precision: 0.523\n",
            "I0630 00:59:46.002844 140438030301056 <ipython-input-11-f314403f781c>:85] step: 600, loss: 1.623, precision: 0.523\n",
            "I0630 00:59:46.004210 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-700\n",
            "I0630 00:59:46.008785 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-700\n",
            "I0630 00:59:49.027488 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.442, precision: 0.553, best precision: 0.553\n",
            "I0630 00:59:49.029664 140438030301056 <ipython-input-11-f314403f781c>:85] step: 700, loss: 1.442, precision: 0.553\n",
            "I0630 00:59:49.033796 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-800\n",
            "I0630 00:59:49.038912 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-800\n",
            "I0630 00:59:52.132982 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.394, precision: 0.567, best precision: 0.567\n",
            "I0630 00:59:52.134978 140438030301056 <ipython-input-11-f314403f781c>:85] step: 800, loss: 1.394, precision: 0.567\n",
            "I0630 00:59:52.136892 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-900\n",
            "I0630 00:59:52.145728 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-900\n",
            "I0630 00:59:55.250341 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.617, precision: 0.596, best precision: 0.596\n",
            "I0630 00:59:55.252300 140438030301056 <ipython-input-11-f314403f781c>:85] step: 900, loss: 1.617, precision: 0.596\n",
            "I0630 00:59:55.253303 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-1000\n",
            "I0630 00:59:55.261404 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-1000\n",
            "I0630 00:59:58.336215 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.329, precision: 0.563, best precision: 0.596\n",
            "I0630 00:59:58.338165 140438030301056 <ipython-input-11-f314403f781c>:85] step: 1000, loss: 1.329, precision: 0.563\n",
            "I0630 00:59:58.342628 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-1100\n",
            "I0630 00:59:58.349065 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-1100\n",
            "I0630 01:00:01.542627 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.421, precision: 0.589, best precision: 0.596\n",
            "I0630 01:00:01.544815 140438030301056 <ipython-input-11-f314403f781c>:85] step: 1100, loss: 1.421, precision: 0.589\n",
            "I0630 01:00:01.549576 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-1200\n",
            "I0630 01:00:01.557456 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-1200\n",
            "I0630 01:00:04.712157 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.377, precision: 0.618, best precision: 0.618\n",
            "I0630 01:00:04.713965 140438030301056 <ipython-input-11-f314403f781c>:85] step: 1200, loss: 1.377, precision: 0.618\n",
            "I0630 01:00:04.715006 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-1300\n",
            "I0630 01:00:04.725414 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-1300\n",
            "I0630 01:00:07.890661 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.177, precision: 0.612, best precision: 0.618\n",
            "I0630 01:00:07.892492 140438030301056 <ipython-input-11-f314403f781c>:85] step: 1300, loss: 1.177, precision: 0.612\n",
            "I0630 01:00:07.897172 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-1400\n",
            "I0630 01:00:07.905344 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-1400\n",
            "I0630 01:00:11.197869 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.186, precision: 0.617, best precision: 0.618\n",
            "I0630 01:00:11.199701 140438030301056 <ipython-input-11-f314403f781c>:85] step: 1400, loss: 1.186, precision: 0.617\n",
            "I0630 01:00:11.200740 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-1500\n",
            "I0630 01:00:11.209934 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-1500\n",
            "I0630 01:00:14.243736 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.371, precision: 0.596, best precision: 0.618\n",
            "I0630 01:00:14.245641 140438030301056 <ipython-input-11-f314403f781c>:85] step: 1500, loss: 1.371, precision: 0.596\n",
            "I0630 01:00:14.250159 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-1600\n",
            "I0630 01:00:14.257816 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-1600\n",
            "I0630 01:00:17.301978 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.244, precision: 0.605, best precision: 0.618\n",
            "I0630 01:00:17.304081 140438030301056 <ipython-input-11-f314403f781c>:85] step: 1600, loss: 1.244, precision: 0.605\n",
            "I0630 01:00:17.308218 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-1700\n",
            "I0630 01:00:17.315028 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-1700\n",
            "I0630 01:00:20.416870 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.097, precision: 0.655, best precision: 0.655\n",
            "I0630 01:00:20.418621 140438030301056 <ipython-input-11-f314403f781c>:85] step: 1700, loss: 1.097, precision: 0.655\n",
            "I0630 01:00:20.419620 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-1800\n",
            "I0630 01:00:20.429553 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-1800\n",
            "I0630 01:00:23.527951 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.088, precision: 0.688, best precision: 0.688\n",
            "I0630 01:00:23.529778 140438030301056 <ipython-input-11-f314403f781c>:85] step: 1800, loss: 1.088, precision: 0.688\n",
            "I0630 01:00:23.530716 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-1900\n",
            "I0630 01:00:23.540735 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-1900\n",
            "I0630 01:00:26.579437 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.154, precision: 0.709, best precision: 0.709\n",
            "I0630 01:00:26.581191 140438030301056 <ipython-input-11-f314403f781c>:85] step: 1900, loss: 1.154, precision: 0.709\n",
            "I0630 01:00:26.585362 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-2000\n",
            "I0630 01:00:26.591528 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-2000\n",
            "I0630 01:00:29.731739 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.328, precision: 0.691, best precision: 0.709\n",
            "I0630 01:00:29.733405 140438030301056 <ipython-input-11-f314403f781c>:85] step: 2000, loss: 1.328, precision: 0.691\n",
            "I0630 01:00:29.734932 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-2100\n",
            "I0630 01:00:29.743663 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-2100\n",
            "I0630 01:00:32.776979 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.123, precision: 0.689, best precision: 0.709\n",
            "I0630 01:00:32.779050 140438030301056 <ipython-input-11-f314403f781c>:85] step: 2100, loss: 1.123, precision: 0.689\n",
            "I0630 01:00:32.783622 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-2200\n",
            "I0630 01:00:32.788691 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-2200\n",
            "I0630 01:00:35.849948 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.103, precision: 0.687, best precision: 0.709\n",
            "I0630 01:00:35.851924 140438030301056 <ipython-input-11-f314403f781c>:85] step: 2200, loss: 1.103, precision: 0.687\n",
            "I0630 01:00:35.852843 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-2300\n",
            "I0630 01:00:35.862829 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-2300\n",
            "I0630 01:00:38.947829 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.102, precision: 0.713, best precision: 0.713\n",
            "I0630 01:00:38.949476 140438030301056 <ipython-input-11-f314403f781c>:85] step: 2300, loss: 1.102, precision: 0.713\n",
            "I0630 01:00:38.951186 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-2400\n",
            "I0630 01:00:38.958476 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-2400\n",
            "I0630 01:00:42.225786 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 0.909, precision: 0.730, best precision: 0.730\n",
            "I0630 01:00:42.227600 140438030301056 <ipython-input-11-f314403f781c>:85] step: 2400, loss: 0.909, precision: 0.730\n",
            "I0630 01:00:42.228944 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-2500\n",
            "I0630 01:00:42.240298 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-2500\n",
            "I0630 01:00:45.251576 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.035, precision: 0.697, best precision: 0.730\n",
            "I0630 01:00:45.253534 140438030301056 <ipython-input-11-f314403f781c>:85] step: 2500, loss: 1.035, precision: 0.697\n",
            "I0630 01:00:45.257498 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-2600\n",
            "I0630 01:00:45.263488 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-2600\n",
            "I0630 01:00:48.291208 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 1.151, precision: 0.708, best precision: 0.730\n",
            "I0630 01:00:48.293318 140438030301056 <ipython-input-11-f314403f781c>:85] step: 2600, loss: 1.151, precision: 0.708\n",
            "I0630 01:00:48.296128 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-2700\n",
            "I0630 01:00:48.303745 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-2700\n",
            "I0630 01:00:51.347678 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 0.875, precision: 0.694, best precision: 0.730\n",
            "I0630 01:00:51.349462 140438030301056 <ipython-input-11-f314403f781c>:85] step: 2700, loss: 0.875, precision: 0.694\n",
            "I0630 01:00:51.350419 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-2800\n",
            "I0630 01:00:51.361016 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-2800\n",
            "I0630 01:00:54.363246 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 0.982, precision: 0.753, best precision: 0.753\n",
            "I0630 01:00:54.364883 140438030301056 <ipython-input-11-f314403f781c>:85] step: 2800, loss: 0.982, precision: 0.753\n",
            "I0630 01:00:54.368220 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-2900\n",
            "I0630 01:00:54.377441 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-2900\n",
            "I0630 01:00:57.409388 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 0.784, precision: 0.759, best precision: 0.759\n",
            "I0630 01:00:57.411521 140438030301056 <ipython-input-11-f314403f781c>:85] step: 2900, loss: 0.784, precision: 0.759\n",
            "I0630 01:00:57.414187 140438030301056 <ipython-input-11-f314403f781c>:53] Loading checkpoint ./train_ckpt/cifar10-train-3000\n",
            "I0630 01:00:57.421118 140438030301056 saver.py:1280] Restoring parameters from ./train_ckpt/cifar10-train-3000\n",
            "I0630 01:01:00.549520 140438030301056 <ipython-input-11-f314403f781c>:81] loss: 0.849, precision: 0.716, best precision: 0.759\n",
            "I0630 01:01:00.551593 140438030301056 <ipython-input-11-f314403f781c>:85] step: 3000, loss: 0.849, precision: 0.716\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlnAphXnUH_M",
        "colab_type": "text"
      },
      "source": [
        "### Display our graph on tensorboard!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LodXGDkTuIR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "f1c01d2e-9c2d-45b1-bf5b-6dd00b566074"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "#run tensorboard\n",
        "LOG_DIR = './tensorboard'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "#run ngrok\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-30 01:01:53--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.173.32.212, 35.173.6.94, 52.72.145.109, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.173.32.212|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17556757 (17M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  16.74M  41.4MB/s    in 0.4s    \n",
            "\n",
            "2019-06-30 01:01:54 (41.4 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [17556757/17556757]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGvkGP2AT97B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "768f994f-0297-412c-a89c-8315686bb6ce"
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://77a0ee77.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79wPverw9aI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}