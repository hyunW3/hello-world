{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "과제 2 ResNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "1y3HiS9MtcxW",
        "KBDxI6zhoyY_",
        "PlnAphXnUH_M"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunW3/kmooc_spark/blob/master/%EA%B3%BC%EC%A0%9C_2_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YWcjwWOJbIu",
        "colab_type": "text"
      },
      "source": [
        "Copyright (C) 2018 Software Platform Lab, Seoul National University\n",
        "\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv1ZcltrbkFI",
        "colab_type": "text"
      },
      "source": [
        "# KMOOC_HW2: Training a ResNet model\n",
        "\n",
        "- Create input pipeline using Tensorflow Dataset API\n",
        "- Define fully connected layer\n",
        "- Define variable update operation using optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu9cVjyfbkz7",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "19f25bac-5d1f-4fef-8f3d-ffece0e20684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#@title Run me to download the CIFAR-10 dataset!\n",
        "# https://blog.shichao.io/2012/10/04/progress_speed_indicator_for_urlretrieve_in_python.html\n",
        "\n",
        "import os, sys, time\n",
        "import tarfile\n",
        "import urllib\n",
        "\n",
        "def reporthook(count, block_size, total_size):\n",
        "  global start_time\n",
        "  if count == 0:\n",
        "    start_time = time.time()\n",
        "    return\n",
        "  duration = time.time() - start_time\n",
        "  progress_size = int(count * block_size)\n",
        "  percent = int(count * block_size * 100 / total_size)\n",
        "  sys.stdout.write('\\r...%d%%, %d MB, %d seconds passed' %\n",
        "                   (percent, progress_size / (1024 * 1024), duration))\n",
        "  sys.stdout.flush()\n",
        "\n",
        "cifar10url = 'https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
        "cifar10 = cifar10url.split('/')[-1]\n",
        "\n",
        "if not os.path.isfile(cifar10):\n",
        "  urllib.urlretrieve(cifar10url, cifar10, reporthook)\n",
        "print()\n",
        "print('Download finished!')\n",
        "\n",
        "cifar10_extracted = 'cifar-10-batches-bin'\n",
        "\n",
        "if not os.path.isdir(cifar10_extracted):\n",
        "  tarfile.open(cifar10, 'r:gz').extractall()\n",
        "print('Uncompression finished!')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "()\n",
            "Download finished!\n",
            "Uncompression finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o4xeczHddip",
        "colab_type": "code",
        "outputId": "3a7ee7bf-7ab2-486f-ee53-c4d61ff72288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir train_ckpt"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘train_ckpt’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBGRBH9fg742",
        "colab_type": "code",
        "outputId": "dcb2d681-d315-4fa3-e99d-ae053f25b550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cifar-10-batches-bin  cifar-10-binary.tar.gz  sample_data  train_ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc0UJQMReBnw",
        "colab_type": "text"
      },
      "source": [
        "## Problem 1. CIFAR10 input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUdHzsItMc4d",
        "colab_type": "code",
        "outputId": "d801f782-0307-4888-ac66-15446b4a2ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"CIFAR dataset input module.\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def build_input(dataset, data_path, batch_size, mode):\n",
        "  \"\"\"Build CIFAR image and labels.\n",
        "\n",
        "  Args:\n",
        "    dataset: Either 'cifar10' or 'cifar100'.\n",
        "    data_path: Filename for data.\n",
        "    batch_size: Input batch size.\n",
        "    mode: Either 'train' or 'eval'.\n",
        "  Returns:\n",
        "    images: Batches of images. [batch_size, image_size, image_size, 3]\n",
        "    labels: Batches of labels. [batch_size, num_classes]\n",
        "  Raises:\n",
        "    ValueError: when the specified dataset is not supported.\n",
        "  \"\"\"\n",
        "  image_size = 32\n",
        "  if dataset == 'cifar10':\n",
        "    label_bytes = 1\n",
        "    label_offset = 0\n",
        "    num_classes = 10\n",
        "  else:\n",
        "    raise ValueError('Not supported dataset %s', dataset)\n",
        "\n",
        "  depth = 3\n",
        "  image_bytes = image_size * image_size * depth\n",
        "  record_bytes = label_bytes + label_offset + image_bytes\n",
        "\n",
        "  def parse_data(value): \n",
        "    # Convert these examples to dense labels and processed images.\n",
        "    record = tf.reshape(tf.decode_raw(value, tf.uint8), [record_bytes])\n",
        "    label = tf.cast(tf.slice(record, [label_offset], [label_bytes]), tf.int32)\n",
        "\n",
        "    # Convert from string to [depth * height * width] to [depth, height, width].\n",
        "    depth_major = tf.reshape(tf.slice(record, [label_offset + label_bytes], [image_bytes]),\n",
        "                           [depth, image_size, image_size])\n",
        "    # Convert from [depth, height, width] to [height, width, depth].\n",
        "    image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n",
        "\n",
        "    if mode == 'train':\n",
        "      image = tf.image.resize_image_with_crop_or_pad(\n",
        "        image, image_size+4, image_size+4)\n",
        "      image = tf.random_crop(image, [image_size, image_size, 3])\n",
        "      image = tf.image.random_flip_left_right(image)\n",
        "      image = tf.image.per_image_standardization(image)\n",
        "\n",
        "    else:\n",
        "      image = tf.image.resize_image_with_crop_or_pad(\n",
        "        image, image_size, image_size)\n",
        "      image = tf.image.per_image_standardization(image)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "  data_files = tf.gfile.Glob(data_path)\n",
        "  # return : A list of strings containing filenames that match the given pattern(s).\n",
        "  data_files.sort()\n",
        "  print(data_files) # list\n",
        "  #############################################################################\n",
        "  #### FIXME: Create an input pipline using tf.data.Dataset and parse_data ####\n",
        "  #############################################################################\n",
        "  #https://nlp.gitbook.io/book/tensorflow/tf.data\n",
        "  #https://stackoverflow.com/questions/54081213/flatten-dataset-of-multiple-files-tensorflow\n",
        "  images = []\n",
        "  labels = []\n",
        "  #print(data_files)\n",
        "  #ValueError: Dimension size must be evenly divisible by 5 but is 3073 for 'Reshape' (op: 'Reshape') with input shapes: [5,?], [1] and with input tensors computed as partial shapes: input[1] = [3073].\n",
        "  for data_file in data_files:\n",
        "    image, label = parse_data(data_file)\n",
        "    images.append(image)\n",
        "    labels.append(label)\n",
        "  \n",
        "  #tf.data.FixedLengthRecordDataset == success\n",
        "  data_files = tf.constant(data_files)\n",
        "  #ds = tf.data.FixedLengthRecordDataset(data_files,record_bytes)\n",
        "  #ds = ds.map(parse_data).repeat().batch(batch_size)\n",
        "  \n",
        "  \n",
        "  ds = tf.data.Dataset.from_tensor_slices(lambda x : (parse_data(x) for x in data_files))\n",
        "  \n",
        "  #print(ds)\n",
        "  #ds = tf.data.TFRecordDataset(data_files).map(parse_data).repeat().batch(batch_size)\n",
        "  \n",
        "  #ds = tf.data.Dataset.from_generator(lambda x :( parse_data(x) for x in data_files),\n",
        "  #                                   (tf.float32, tf.float32),\n",
        "  #                                   (tf.TensorShape([32,32,3]), tf.TensorShape([1,]) \n",
        "  #                                    ))\n",
        "  \n",
        "  \n",
        "  #ds = tf.data.Dataset.from_tensor_slices((data_files,labels))\n",
        "  # out of range\n",
        "  \n",
        "  #ds = tf.data.Dataset.from_tensor_slices(data_files).map(parse_data)\n",
        "  # assertionError\n",
        "  \n",
        "  #ds = tf.data.Dataset.from_tensor_slices((images,labels))\n",
        "  #Failed to create a one-shot iterator for a dataset. `Dataset.make_one_shot_iterator()` does not support datasets that capture stateful objects\n",
        "  \n",
        "\n",
        "  \n",
        "  iterator = ds.make_one_shot_iterator()\n",
        "  #iterator = ds.make_initializable_iterator()\n",
        "  images, labels = iterator.get_next()\n",
        "  print(\"========image_size : \"+ str(image_size) +\"=======\")\n",
        "  print(images.shape[0])\n",
        "  print( images.shape[1] )\n",
        "  print(images.shape[2])\n",
        "  print(images.shape[3])\n",
        "  \n",
        "  #original\n",
        "  assert images.shape[1] ==  images.shape[2] == image_size\n",
        "  assert images.shape[3] == depth\n",
        "  \n",
        "  #alternative\n",
        "  #assert images.shape[0] == images.shape[1] == image_size\n",
        "  #assert images.shape[2] == depth\n",
        "  \n",
        "  assert labels.shape[1] == 1\n",
        "  \n",
        "  images = tf.reshape(images, [batch_size, image_size, image_size, depth])\n",
        "  labels = tf.reshape(labels, [batch_size,  +1])\n",
        "  indices = tf.reshape(tf.range(0, batch_size, 1), [batch_size, 1])\n",
        "  labels = tf.sparse_to_dense(\n",
        "      tf.concat(values=[indices, labels], axis=1),\n",
        "      [batch_size, num_classes], 1.0, 0.0) \n",
        "  return images, labels\n",
        "\n",
        "train_data_path = './cifar-10-batches-bin/data_batch*'\n",
        "batch_size = 128\n",
        "with tf.Graph().as_default():\n",
        "    images, labels = build_input(\n",
        "        'cifar10', train_data_path, batch_size, 'train')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['./cifar-10-batches-bin/data_batch_1.bin', './cifar-10-batches-bin/data_batch_2.bin', './cifar-10-batches-bin/data_batch_3.bin', './cifar-10-batches-bin/data_batch_4.bin', './cifar-10-batches-bin/data_batch_5.bin']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-79b3ac9f1f5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     images, labels = build_input(\n\u001b[0;32m--> 135\u001b[0;31m         'cifar10', train_data_path, batch_size, 'train')\n\u001b[0m",
            "\u001b[0;32m<ipython-input-79-79b3ac9f1f5f>\u001b[0m in \u001b[0;36mbuild_input\u001b[0;34m(dataset, data_path, batch_size, mode)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m   \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparse_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;31m#print(ds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/data/ops/dataset_ops.pyc\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   1683\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDatasetV1Adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/data/ops/dataset_ops.pyc\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    362\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \"\"\"\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/data/ops/dataset_ops.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m   2219\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2220\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tensors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2221\u001b[0;31m       \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2223\u001b[0m     \u001b[0mbatched_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/data/util/structure.pyc\u001b[0m in \u001b[0;36mnormalize_tensors\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mprepared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mprepared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"component_%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[1;32m   1085\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[1;32m   1086\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[0;32m-> 1087\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1143\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    303\u001b[0m                                          as_ref=False):\n\u001b[1;32m    304\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    244\u001b[0m   \"\"\"\n\u001b[1;32m    245\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 246\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    282\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m    283\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    285\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m   const_tensor = g.create_op(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.pyc\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    560\u001b[0m       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\u001b[1;32m    561\u001b[0m                       \u001b[0;34m\"Contents: %s. Consider casting elements to a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m                       \"supported type.\" % (type(values), values))\n\u001b[0m\u001b[1;32m    563\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Failed to convert object of type <type 'function'> to Tensor. Contents: <function <lambda> at 0x7f2d8c5e07d0>. Consider casting elements to a supported type."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ613UElFNNn",
        "colab_type": "text"
      },
      "source": [
        "   \n",
        "     \n",
        "\n",
        "data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
        "\n",
        "labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47vs_zxwbzgS",
        "colab_type": "text"
      },
      "source": [
        "##Problem 2. Define FC layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAwLxH90Ormy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fully_connected(batch_size, x, out_dim):\n",
        "    \"\"\"FullyConnected layer for final output.\"\"\"\n",
        "    x = tf.reshape(x, [batch_size, -1])\n",
        "    w = tf.get_variable(\n",
        "        'DW', [x.get_shape()[1], out_dim],\n",
        "        initializer=tf.uniform_unit_scaling_initializer(factor=1.0))\n",
        "    \n",
        "    ################################################\n",
        "    #### FIXME: Create an variable 'b'          ####\n",
        "    #### HINT: name: 'biases', shape: [out_dim] ####\n",
        "    ####       use constant_initializer         ####\n",
        "    ################################################\n",
        "    b = tf.get_variable(\n",
        "      'biases', shape=[out_dim], initializer= tf.constant_initializer(0.0))\n",
        "    \n",
        "    ##################################################\n",
        "    #### FIXME: Create an xw_plus_b op            ####\n",
        "    #### HINT: xw+b (xw is matrix multiplication) ####\n",
        "    ##################################################\n",
        "  \n",
        "    xw_plus_b = tf.matmul(x,w)+b\n",
        "    return xw_plus_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7bOoR5QUTkE",
        "colab_type": "text"
      },
      "source": [
        "## Problem 3. Define optimizer and update operation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN9vleA0UXow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_train_op(loss, lrn_rate):\n",
        "    \"\"\"Build training specific ops for the graph.\"\"\"\n",
        "\n",
        "    #########################################################################\n",
        "    #### FIXME: Create an optimizer using self.lrn_rate as learning rate ####\n",
        "    #########################################################################\n",
        "    update_op = tf.train.GradientDescentOptimizer(learning_rate=lrn_rate).minimize(loss)\n",
        "    \n",
        "    return update_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1UUs2SrWwye",
        "colab_type": "text"
      },
      "source": [
        "## Define ResNet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL5o4WI7b2-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"ResNet model.\n",
        "\n",
        "Related papers:\n",
        "https://arxiv.org/pdf/1603.05027v2.pdf\n",
        "https://arxiv.org/pdf/1512.03385v1.pdf\n",
        "https://arxiv.org/pdf/1605.07146v1.pdf\n",
        "\"\"\"\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import six\n",
        "\n",
        "from tensorflow.python.training import moving_averages\n",
        "\n",
        "\n",
        "HParams = namedtuple('HParams',\n",
        "                     'batch_size, num_classes, min_lrn_rate, lrn_rate, '\n",
        "                     'num_residual_units, use_bottleneck, weight_decay_rate, '\n",
        "                     'relu_leakiness')\n",
        "\n",
        "\n",
        "class ResNet(object):\n",
        "  \"\"\"ResNet model.\"\"\"\n",
        "\n",
        "  def __init__(self, hps, images, labels, mode):\n",
        "    \"\"\"ResNet constructor.\n",
        "\n",
        "    Args:\n",
        "      hps: Hyperparameters.\n",
        "      images: Batches of images. [batch_size, image_size, image_size, 3]\n",
        "      labels: Batches of labels. [batch_size, num_classes]\n",
        "      mode: One of 'train' and 'eval'.\n",
        "    \"\"\"\n",
        "    self.hps = hps\n",
        "    self._images = images\n",
        "    self.labels = labels\n",
        "    self.mode = mode\n",
        "\n",
        "    self._extra_train_ops = []\n",
        "\n",
        "  def build_graph(self):\n",
        "    \"\"\"Build a whole graph for the model.\"\"\"\n",
        "    self.global_step = tf.train.get_or_create_global_step()\n",
        "    self._build_model()\n",
        "    if self.mode == 'train':\n",
        "      self.lrn_rate =  tf.constant(self.hps.lrn_rate, tf.float32)\n",
        "      update_op = build_train_op(self.cost, self.lrn_rate)\n",
        "      \n",
        "      with tf.control_dependencies([update_op]):\n",
        "          apply_op = tf.assign_add(self.global_step, 1)\n",
        "   \n",
        "      train_ops = [apply_op] + self._extra_train_ops\n",
        "      self.train_op = tf.group(*train_ops)\n",
        "      \n",
        "    self.summaries = tf.summary.merge_all()\n",
        "\n",
        "  def _stride_arr(self, stride):\n",
        "    \"\"\"Map a stride scalar to the stride array for tf.nn.conv2d.\"\"\"\n",
        "    return [1, stride, stride, 1]\n",
        "\n",
        "  def _build_model(self):\n",
        "    \"\"\"Build the core model within the graph.\"\"\"\n",
        "    with tf.variable_scope('init'):\n",
        "      x = self._images\n",
        "      x = self._conv('init_conv', x, 3, 3, 16, self._stride_arr(1))\n",
        "\n",
        "    strides = [1, 2, 2]\n",
        "    activate_before_residual = [True, False, False]\n",
        "    if self.hps.use_bottleneck:\n",
        "      res_func = self._bottleneck_residual\n",
        "      filters = [16, 64, 128, 256]\n",
        "    else:\n",
        "      res_func = self._residual\n",
        "      filters = [16, 16, 32, 64]\n",
        "      # Uncomment the following codes to use w28-10 wide residual network.\n",
        "      # It is more memory efficient than very deep residual network and has\n",
        "      # comparably good performance.\n",
        "      # https://arxiv.org/pdf/1605.07146v1.pdf\n",
        "      # filters = [16, 160, 320, 640]\n",
        "      # Update hps.num_residual_units to 4\n",
        "\n",
        "    with tf.variable_scope('unit_1_0'):\n",
        "      x = res_func(x, filters[0], filters[1], self._stride_arr(strides[0]),\n",
        "                   activate_before_residual[0])\n",
        "    for i in six.moves.range(1, self.hps.num_residual_units):\n",
        "      with tf.variable_scope('unit_1_%d' % i):\n",
        "        x = res_func(x, filters[1], filters[1], self._stride_arr(1), False)\n",
        "\n",
        "    with tf.variable_scope('unit_2_0'):\n",
        "      x = res_func(x, filters[1], filters[2], self._stride_arr(strides[1]),\n",
        "                   activate_before_residual[1])\n",
        "    for i in six.moves.range(1, self.hps.num_residual_units):\n",
        "      with tf.variable_scope('unit_2_%d' % i):\n",
        "        x = res_func(x, filters[2], filters[2], self._stride_arr(1), False)\n",
        "\n",
        "    with tf.variable_scope('unit_3_0'):\n",
        "      x = res_func(x, filters[2], filters[3], self._stride_arr(strides[2]),\n",
        "                   activate_before_residual[2])\n",
        "    for i in six.moves.range(1, self.hps.num_residual_units):\n",
        "      with tf.variable_scope('unit_3_%d' % i):\n",
        "        x = res_func(x, filters[3], filters[3], self._stride_arr(1), False)\n",
        "\n",
        "    with tf.variable_scope('unit_last'):\n",
        "      x = self._batch_norm('final_bn', x)\n",
        "      x = self._relu(x, self.hps.relu_leakiness)\n",
        "      x = self._global_avg_pool(x)\n",
        "\n",
        "    with tf.variable_scope('logit'):\n",
        "      logits = fully_connected(self.hps.batch_size, x, self.hps.num_classes)\n",
        "      self.predictions = tf.nn.softmax(logits)\n",
        "\n",
        "    with tf.variable_scope('costs'):\n",
        "      xent = tf.nn.softmax_cross_entropy_with_logits(\n",
        "          logits=logits, labels=self.labels)\n",
        "      self.cost = tf.reduce_mean(xent, name='xent')\n",
        "      self.cost += self._decay()\n",
        "\n",
        "      tf.summary.scalar('cost', self.cost)\n",
        "\n",
        "  # TODO(xpan): Consider batch_norm in contrib/layers/python/layers/layers.py\n",
        "  def _batch_norm(self, name, x):\n",
        "    \"\"\"Batch normalization.\"\"\"\n",
        "    with tf.variable_scope(name):\n",
        "      params_shape = [x.get_shape()[-1]]\n",
        "\n",
        "      beta = tf.get_variable(\n",
        "          'beta', params_shape, tf.float32,\n",
        "          initializer=tf.constant_initializer(0.0, tf.float32))\n",
        "      gamma = tf.get_variable(\n",
        "          'gamma', params_shape, tf.float32,\n",
        "          initializer=tf.constant_initializer(1.0, tf.float32))\n",
        "\n",
        "      if self.mode == 'train':\n",
        "        mean, variance = tf.nn.moments(x, [0, 1, 2], name='moments')\n",
        "\n",
        "        moving_mean = tf.get_variable(\n",
        "            'moving_mean', params_shape, tf.float32,\n",
        "            initializer=tf.constant_initializer(0.0, tf.float32),\n",
        "            trainable=False)\n",
        "        moving_variance = tf.get_variable(\n",
        "            'moving_variance', params_shape, tf.float32,\n",
        "            initializer=tf.constant_initializer(1.0, tf.float32),\n",
        "            trainable=False)\n",
        "\n",
        "        self._extra_train_ops.append(moving_averages.assign_moving_average(\n",
        "            moving_mean, mean, 0.9))\n",
        "        self._extra_train_ops.append(moving_averages.assign_moving_average(\n",
        "            moving_variance, variance, 0.9))\n",
        "      else:\n",
        "        mean = tf.get_variable(\n",
        "            'moving_mean', params_shape, tf.float32,\n",
        "            initializer=tf.constant_initializer(0.0, tf.float32),\n",
        "            trainable=False)\n",
        "        variance = tf.get_variable(\n",
        "            'moving_variance', params_shape, tf.float32,\n",
        "            initializer=tf.constant_initializer(1.0, tf.float32),\n",
        "            trainable=False)\n",
        "        tf.summary.histogram(mean.op.name, mean)\n",
        "        tf.summary.histogram(variance.op.name, variance)\n",
        "      # epsilon used to be 1e-5. Maybe 0.001 solves NaN problem in deeper net.\n",
        "      y = tf.nn.batch_normalization(\n",
        "          x, mean, variance, beta, gamma, 0.001)\n",
        "      y.set_shape(x.get_shape())\n",
        "      return y\n",
        "\n",
        "  def _residual(self, x, in_filter, out_filter, stride,\n",
        "                activate_before_residual=False):\n",
        "    \"\"\"Residual unit with 2 sub layers.\"\"\"\n",
        "    if activate_before_residual:\n",
        "      with tf.variable_scope('shared_activation'):\n",
        "        x = self._batch_norm('init_bn', x)\n",
        "        x = self._relu(x, self.hps.relu_leakiness)\n",
        "        orig_x = x\n",
        "    else:\n",
        "      with tf.variable_scope('residual_only_activation'):\n",
        "        orig_x = x\n",
        "        x = self._batch_norm('init_bn', x)\n",
        "        x = self._relu(x, self.hps.relu_leakiness)\n",
        "\n",
        "    with tf.variable_scope('sub1'):\n",
        "      x = self._conv('conv1', x, 3, in_filter, out_filter, stride)\n",
        "\n",
        "    with tf.variable_scope('sub2'):\n",
        "      x = self._batch_norm('bn2', x)\n",
        "      x = self._relu(x, self.hps.relu_leakiness)\n",
        "      x = self._conv('conv2', x, 3, out_filter, out_filter, [1, 1, 1, 1])\n",
        "\n",
        "    with tf.variable_scope('sub_add'):\n",
        "      if in_filter != out_filter:\n",
        "        orig_x = tf.nn.avg_pool(orig_x, stride, stride, 'VALID')\n",
        "        orig_x = tf.pad(\n",
        "            orig_x, [[0, 0], [0, 0], [0, 0],\n",
        "                     [(out_filter-in_filter)//2, (out_filter-in_filter)//2]])\n",
        "      x += orig_x\n",
        "\n",
        "    tf.logging.debug('image after unit %s', x.get_shape())\n",
        "    return x\n",
        "\n",
        "  def _bottleneck_residual(self, x, in_filter, out_filter, stride,\n",
        "                           activate_before_residual=False):\n",
        "    \"\"\"Bottleneck residual unit with 3 sub layers.\"\"\"\n",
        "    if activate_before_residual:\n",
        "      with tf.variable_scope('common_bn_relu'):\n",
        "        x = self._batch_norm('init_bn', x)\n",
        "        x = self._relu(x, self.hps.relu_leakiness)\n",
        "        orig_x = x\n",
        "    else:\n",
        "      with tf.variable_scope('residual_bn_relu'):\n",
        "        orig_x = x\n",
        "        x = self._batch_norm('init_bn', x)\n",
        "        x = self._relu(x, self.hps.relu_leakiness)\n",
        "\n",
        "    with tf.variable_scope('sub1'):\n",
        "      x = self._conv('conv1', x, 1, in_filter, out_filter/4, stride)\n",
        "\n",
        "    with tf.variable_scope('sub2'):\n",
        "      x = self._batch_norm('bn2', x)\n",
        "      x = self._relu(x, self.hps.relu_leakiness)\n",
        "      x = self._conv('conv2', x, 3, out_filter/4, out_filter/4, [1, 1, 1, 1])\n",
        "\n",
        "    with tf.variable_scope('sub3'):\n",
        "      x = self._batch_norm('bn3', x)\n",
        "      x = self._relu(x, self.hps.relu_leakiness)\n",
        "      x = self._conv('conv3', x, 1, out_filter/4, out_filter, [1, 1, 1, 1])\n",
        "\n",
        "    with tf.variable_scope('sub_add'):\n",
        "      if in_filter != out_filter:\n",
        "        orig_x = self._conv('project', orig_x, 1, in_filter, out_filter, stride)\n",
        "      x += orig_x\n",
        "\n",
        "    tf.logging.info('image after unit %s', x.get_shape())\n",
        "    return x\n",
        "\n",
        "  def _decay(self):\n",
        "    \"\"\"L2 weight decay loss.\"\"\"\n",
        "    costs = []\n",
        "    for var in tf.trainable_variables():\n",
        "      if var.op.name.find(r'DW') > 0:\n",
        "        costs.append(tf.nn.l2_loss(var))\n",
        "        # tf.summary.histogram(var.op.name, var)\n",
        "\n",
        "    return tf.multiply(self.hps.weight_decay_rate, tf.add_n(costs))\n",
        "\n",
        "  def _conv(self, name, x, filter_size, in_filters, out_filters, strides):\n",
        "    \"\"\"Convolution.\"\"\"\n",
        "    with tf.variable_scope(name):\n",
        "      n = filter_size * filter_size * out_filters\n",
        "      kernel = tf.get_variable(\n",
        "          'DW', [filter_size, filter_size, in_filters, out_filters],\n",
        "          tf.float32, initializer=tf.random_normal_initializer(\n",
        "              stddev=np.sqrt(2.0/n)))\n",
        "      return tf.nn.conv2d(x, kernel, strides, padding='SAME')\n",
        "\n",
        "  def _relu(self, x, leakiness=0.0):\n",
        "    \"\"\"Relu, with optional leaky support.\"\"\"\n",
        "    return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')\n",
        "\n",
        "  def _global_avg_pool(self, x):\n",
        "    assert x.get_shape().ndims == 4\n",
        "    return tf.reduce_mean(x, [1, 2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYszRyxjb2aZ",
        "colab_type": "text"
      },
      "source": [
        "## Train ResNet model\n",
        "Do not be frightened if you face such an error: \n",
        "`An exception has occurred, use %tb to see the full traceback.  SystemExit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR7jlpOUdQhl",
        "colab_type": "code",
        "outputId": "bba2591e-26a2-417f-bfb9-63b8fe0b79c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"ResNet Train/Eval module.\n",
        "\"\"\"\n",
        "import time\n",
        "import six\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Global\n",
        "train_data_path = './cifar-10-batches-bin/data_batch*'\n",
        "image_size = 32\n",
        "ckpt_dir = './train_ckpt'\n",
        "ckpt_prefix = ckpt_dir + '/cifar10-train'\n",
        "\n",
        "def train(hps):\n",
        "  \"\"\"Training loop.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    images, labels = build_input(\n",
        "    'cifar10', train_data_path, hps.batch_size, 'train')\n",
        "    model = ResNet(hps, images, labels, 'train')\n",
        "    model.build_graph()\n",
        "\n",
        "    truth = tf.argmax(model.labels, axis=1)\n",
        "    predictions = tf.argmax(model.predictions, axis=1)\n",
        "    precision = tf.reduce_mean(tf.to_float(tf.equal(predictions, truth)))\n",
        "  \n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    saver = tf.train.Saver(max_to_keep=10000)\n",
        "  \n",
        "    with tf.Session() as sess:\n",
        "      sess.run(init)\n",
        "      for i in range(3001):\n",
        "        _, global_step, cost, precision_ = \\\n",
        "          sess.run([model.train_op, model.global_step, model.cost, precision])\n",
        "    \n",
        "        if global_step % 100 == 0:\n",
        "          print('step: %d, loss: %.3f, precision: %.3f' % (global_step, cost, precision_))\n",
        "\n",
        "          saver.save(sess, ckpt_prefix, global_step=i)\n",
        "\n",
        "        \n",
        "def main(_):\n",
        "  batch_size = 128\n",
        "\n",
        "  hps = HParams(batch_size=batch_size,\n",
        "                             num_classes=10,\n",
        "                             min_lrn_rate=0.0001,\n",
        "                             lrn_rate=0.1,\n",
        "                             num_residual_units=5,\n",
        "                             use_bottleneck=False,\n",
        "                             weight_decay_rate=0.0002,\n",
        "                             relu_leakiness=0.1)\n",
        "\n",
        "  train(hps)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.run()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['./cifar-10-batches-bin/data_batch_1.bin', './cifar-10-batches-bin/data_batch_2.bin', './cifar-10-batches-bin/data_batch_3.bin', './cifar-10-batches-bin/data_batch_4.bin', './cifar-10-batches-bin/data_batch_5.bin']\n",
            "========image_size : 32=======\n",
            "?\n",
            "32\n",
            "32\n",
            "3\n",
            "step: 0, loss: 2.638, precision: 0.031\n",
            "step: 100, loss: 1.892, precision: 0.328\n",
            "step: 200, loss: 1.867, precision: 0.445\n",
            "step: 300, loss: 1.709, precision: 0.484\n",
            "step: 400, loss: 1.522, precision: 0.516\n",
            "step: 500, loss: 1.484, precision: 0.555\n",
            "step: 600, loss: 1.650, precision: 0.492\n",
            "step: 700, loss: 1.327, precision: 0.609\n",
            "step: 800, loss: 1.200, precision: 0.664\n",
            "step: 900, loss: 1.251, precision: 0.625\n",
            "step: 1000, loss: 1.328, precision: 0.602\n",
            "step: 1100, loss: 1.262, precision: 0.617\n",
            "step: 1200, loss: 0.997, precision: 0.727\n",
            "step: 1300, loss: 1.161, precision: 0.648\n",
            "step: 1400, loss: 1.093, precision: 0.703\n",
            "step: 1500, loss: 1.089, precision: 0.734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-74d36d64d250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/absl/app.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/absl/app.pyc\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-74d36d64d250>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m     54\u001b[0m                              relu_leakiness=0.1)\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-74d36d64d250>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(hps)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_\u001b[0m \u001b[0;34m=\u001b[0m           \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y3HiS9MtcxW",
        "colab_type": "text"
      },
      "source": [
        "### You can see *cifar10-train-0~3000* checkpoint files when you run following code, after you train model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg6E42w4oTB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls train_ckpt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBDxI6zhoyY_",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate trained ResNet model\n",
        "\n",
        "Before you run this code\n",
        "click Runtime->**restart runtime**\n",
        "\n",
        "(If you want to erase all the local files, then click *RESET ALL RUNTIMES* or **DO NOT CLICK!**)\n",
        "\n",
        "and restart **Define the Resnet50 Model**,  **CIFAR10 input**\n",
        "\n",
        "Do not be frightened if you face such an error: \n",
        "`An exception has occurred, use %tb to see the full traceback.  SystemExit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj2qiublo3hU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf './tensorboard'\n",
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"ResNet Train/Eval module.\n",
        "\"\"\"\n",
        "import time\n",
        "import six\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "eval_data_path = './cifar-10-batches-bin/test_batch.bin'\n",
        "ckpt_dir = './train_ckpt'\n",
        "tensorboard_path = './tensorboard'\n",
        "\n",
        "def evaluate(hps):\n",
        "  \"\"\"Eval loop.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    images, labels = build_input(\n",
        "      'cifar10', './cifar-10-batches-bin/test_batch.bin', hps.batch_size, 'eval')\n",
        "    model = ResNet(hps, images, labels, 'eval')\n",
        "    model.build_graph()\n",
        "\n",
        "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
        "  \n",
        "    saver = tf.train.Saver() \n",
        "    \n",
        "    summary_writer = tf.summary.FileWriter('./tensorboard', sess.graph)\n",
        "\n",
        "    try:\n",
        "      ckpt_state = tf.train.get_checkpoint_state(ckpt_dir)\n",
        "    except tf.errors.OutOfRangeError as e:\n",
        "      tf.logging.error('Cannot restore checkpoint: %s', e)\n",
        "    if not (ckpt_state):\n",
        "      tf.logging.info('No model to eval yet at %s', ckpt_dir)\n",
        "    \n",
        "    best_precision = 0.\n",
        "    for i in range(len(ckpt_state.all_model_checkpoint_paths)):\n",
        "      tf.logging.info('Loading checkpoint %s', ckpt_state.all_model_checkpoint_paths[i])\n",
        "      saver.restore(sess, ckpt_state.all_model_checkpoint_paths[i])\n",
        "      total_prediction, correct_prediction = 0, 0\n",
        "\n",
        "      for _ in six.moves.range(100):\n",
        "        (summaries, loss, predictions, truth, train_step) = sess.run(\n",
        "          [model.summaries, model.cost, model.predictions,\n",
        "           model.labels, model.global_step])\n",
        "\n",
        "        truth = np.argmax(truth, axis=1)\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        correct_prediction += np.sum(truth == predictions)\n",
        "        total_prediction += predictions.shape[0]\n",
        "\n",
        "      precision = 1.0 * correct_prediction / total_prediction\n",
        "      best_precision = max(precision, best_precision)\n",
        "    \n",
        "      precision_summ = tf.Summary()\n",
        "      precision_summ.value.add(\n",
        "        tag='Precision', simple_value=precision)\n",
        "      summary_writer.add_summary(precision_summ, train_step)\n",
        "      best_precision_summ = tf.Summary()\n",
        "      best_precision_summ.value.add(\n",
        "        tag='Best Precision', simple_value=best_precision)\n",
        "      summary_writer.add_summary(best_precision_summ, train_step)\n",
        "      summary_writer.add_summary(summaries, train_step)\n",
        "     \n",
        "      tf.logging.info('loss: %.3f, precision: %.3f, best precision: %.3f' %\n",
        "                      (loss, precision, best_precision))\n",
        "      summary_writer.flush()\n",
        "\n",
        "      tf.logging.info('step: %d, loss: %.3f, precision: %.3f' %\n",
        "                      (i * 100, loss, precision))\n",
        "\n",
        "\n",
        "def main(_):\n",
        "\n",
        "  hps = HParams(batch_size=100,\n",
        "                num_classes=10,\n",
        "                min_lrn_rate=0.0001,\n",
        "                lrn_rate=0.1,\n",
        "                num_residual_units=5,\n",
        "                use_bottleneck=False,\n",
        "                weight_decay_rate=0.0002,\n",
        "                relu_leakiness=0.1)\n",
        "\n",
        "  evaluate(hps)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlnAphXnUH_M",
        "colab_type": "text"
      },
      "source": [
        "### Display our graph on tensorboard!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LodXGDkTuIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "#run tensorboard\n",
        "LOG_DIR = './tensorboard'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "#run ngrok\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGvkGP2AT97B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79wPverw9aI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}