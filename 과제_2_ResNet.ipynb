{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "과제 2 ResNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "HYszRyxjb2aZ",
        "1y3HiS9MtcxW",
        "KBDxI6zhoyY_",
        "PlnAphXnUH_M"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunW3/kmooc_spark/blob/master/%EA%B3%BC%EC%A0%9C_2_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YWcjwWOJbIu",
        "colab_type": "text"
      },
      "source": [
        "Copyright (C) 2018 Software Platform Lab, Seoul National University\n",
        "\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv1ZcltrbkFI",
        "colab_type": "text"
      },
      "source": [
        "# KMOOC_HW2: Training a ResNet model\n",
        "\n",
        "- Create input pipeline using Tensorflow Dataset API\n",
        "- Define fully connected layer\n",
        "- Define variable update operation using optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu9cVjyfbkz7",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "8c389575-ed76-4bab-c7ee-7f38deabbccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#@title Run me to download the CIFAR-10 dataset!\n",
        "# https://blog.shichao.io/2012/10/04/progress_speed_indicator_for_urlretrieve_in_python.html\n",
        "\n",
        "import os, sys, time\n",
        "import tarfile\n",
        "import urllib\n",
        "\n",
        "def reporthook(count, block_size, total_size):\n",
        "  global start_time\n",
        "  if count == 0:\n",
        "    start_time = time.time()\n",
        "    return\n",
        "  duration = time.time() - start_time\n",
        "  progress_size = int(count * block_size)\n",
        "  percent = int(count * block_size * 100 / total_size)\n",
        "  sys.stdout.write('\\r...%d%%, %d MB, %d seconds passed' %\n",
        "                   (percent, progress_size / (1024 * 1024), duration))\n",
        "  sys.stdout.flush()\n",
        "\n",
        "cifar10url = 'https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
        "cifar10 = cifar10url.split('/')[-1]\n",
        "\n",
        "if not os.path.isfile(cifar10):\n",
        "  urllib.urlretrieve(cifar10url, cifar10, reporthook)\n",
        "print()\n",
        "print('Download finished!')\n",
        "\n",
        "cifar10_extracted = 'cifar-10-batches-bin'\n",
        "\n",
        "if not os.path.isdir(cifar10_extracted):\n",
        "  tarfile.open(cifar10, 'r:gz').extractall()\n",
        "print('Uncompression finished!')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "()\n",
            "Download finished!\n",
            "Uncompression finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o4xeczHddip",
        "colab_type": "code",
        "outputId": "c4ed5447-64cd-44e8-8873-93ec066dbe8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "!mkdir train_ckpt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘train_ckpt’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBGRBH9fg742",
        "colab_type": "code",
        "outputId": "40f88eff-680e-43d1-88ce-9374c8956c4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cifar-10-batches-bin  cifar-10-binary.tar.gz  sample_data  train_ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc0UJQMReBnw",
        "colab_type": "text"
      },
      "source": [
        "## Problem 1. CIFAR10 input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUdHzsItMc4d",
        "colab_type": "code",
        "outputId": "f457aa4e-b3a6-40bf-cbd3-92b6da7e5269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"CIFAR dataset input module.\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def build_input(dataset, data_path, batch_size, mode):\n",
        "  \"\"\"Build CIFAR image and labels.\n",
        "\n",
        "  Args:\n",
        "    dataset: Either 'cifar10' or 'cifar100'.\n",
        "    data_path: Filename for data.\n",
        "    batch_size: Input batch size.\n",
        "    mode: Either 'train' or 'eval'.\n",
        "  Returns:\n",
        "    images: Batches of images. [batch_size, image_size, image_size, 3]\n",
        "    labels: Batches of labels. [batch_size, num_classes]\n",
        "  Raises:\n",
        "    ValueError: when the specified dataset is not supported.\n",
        "  \"\"\"\n",
        "  image_size = 32\n",
        "  if dataset == 'cifar10':\n",
        "    label_bytes = 1\n",
        "    label_offset = 0\n",
        "    num_classes = 10\n",
        "  else:\n",
        "    raise ValueError('Not supported dataset %s', dataset)\n",
        "\n",
        "  depth = 3\n",
        "  image_bytes = image_size * image_size * depth\n",
        "  record_bytes = label_bytes + label_offset + image_bytes\n",
        "\n",
        "  def parse_data(value): \n",
        "    # Convert these examples to dense labels and processed images.\n",
        "    record = tf.reshape(tf.decode_raw(value, tf.uint8), [record_bytes])\n",
        "    label = tf.cast(tf.slice(record, [label_offset], [label_bytes]), tf.int32)\n",
        "\n",
        "    # Convert from string to [depth * height * width] to [depth, height, width].\n",
        "    depth_major = tf.reshape(tf.slice(record, [label_offset + label_bytes], [image_bytes]),\n",
        "                           [depth, image_size, image_size])\n",
        "    # Convert from [depth, height, width] to [height, width, depth].\n",
        "    image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n",
        "\n",
        "    if mode == 'train':\n",
        "      image = tf.image.resize_image_with_crop_or_pad(\n",
        "        image, image_size+4, image_size+4)\n",
        "      image = tf.random_crop(image, [image_size, image_size, 3])\n",
        "      image = tf.image.random_flip_left_right(image)\n",
        "      image = tf.image.per_image_standardization(image)\n",
        "\n",
        "    else:\n",
        "      image = tf.image.resize_image_with_crop_or_pad(\n",
        "        image, image_size, image_size)\n",
        "      image = tf.image.per_image_standardization(image)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "  data_files = tf.gfile.Glob(data_path)\n",
        "  # return : A list of strings containing filenames that match the given pattern(s).\n",
        "  data_files.sort()\n",
        "  \n",
        "  #############################################################################\n",
        "  #### FIXME: Create an input pipline using tf.data.Dataset and parse_data ####\n",
        "  #############################################################################\n",
        "  images = []\n",
        "  labels = []\n",
        "  for i in data_files:\n",
        "    image, label = parse_data(i)\n",
        "    images.append(image)\n",
        "    labels.append(label)\n",
        "  print(images)\n",
        "  print(labels)\n",
        "  ds = tf.data.Dataset.from_tensor_slices((data_files,labels))#.repeat()\n",
        "  print(ds)\n",
        "\n",
        "  iterator = ds.make_one_shot_iterator()\n",
        "  images, labels = iterator.get_next()\n",
        "  \n",
        "  assert images.shape[1] ==  images.shape[2] == image_size\n",
        "  assert images.shape[3] == depth\n",
        "  assert labels.shape[1] == 1\n",
        "  \n",
        "  images = tf.reshape(images, [batch_size, image_size, image_size, depth])\n",
        "  labels = tf.reshape(labels, [batch_size, 1])\n",
        "  indices = tf.reshape(tf.range(0, batch_size, 1), [batch_size, 1])\n",
        "  labels = tf.sparse_to_dense(\n",
        "      tf.concat(values=[indices, labels], axis=1),\n",
        "      [batch_size, num_classes], 1.0, 0.0) \n",
        "  return images, labels\n",
        "\n",
        "train_data_path = './cifar-10-batches-bin/data_batch*'\n",
        "batch_size = 128\n",
        "with tf.Graph().as_default():\n",
        "    images, labels = build_input(\n",
        "        'cifar10', train_data_path, batch_size, 'train')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'per_image_standardization:0' shape=(32, 32, 3) dtype=float32>, <tf.Tensor 'per_image_standardization_1:0' shape=(32, 32, 3) dtype=float32>, <tf.Tensor 'per_image_standardization_2:0' shape=(32, 32, 3) dtype=float32>, <tf.Tensor 'per_image_standardization_3:0' shape=(32, 32, 3) dtype=float32>, <tf.Tensor 'per_image_standardization_4:0' shape=(32, 32, 3) dtype=float32>]\n",
            "[<tf.Tensor 'Cast:0' shape=(1,) dtype=int32>, <tf.Tensor 'Cast_2:0' shape=(1,) dtype=int32>, <tf.Tensor 'Cast_4:0' shape=(1,) dtype=int32>, <tf.Tensor 'Cast_6:0' shape=(1,) dtype=int32>, <tf.Tensor 'Cast_8:0' shape=(1,) dtype=int32>]\n",
            "<DatasetV1Adapter shapes: ((), (1,)), types: (tf.string, tf.int32)>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-343c40f3c17c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     images, labels = build_input(\n\u001b[0;32m---> 95\u001b[0;31m         'cifar10', train_data_path, batch_size, 'train')\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-343c40f3c17c>\u001b[0m in \u001b[0;36mbuild_input\u001b[0;34m(dataset, data_path, batch_size, mode)\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m   \u001b[0;32massert\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m  \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ613UElFNNn",
        "colab_type": "text"
      },
      "source": [
        "   \n",
        "     \n",
        "  print(data_files)\n",
        "  ds = tf.data.Dataset.from_tensor_slices(data_files)\n",
        "  print(ds)\n",
        "  ds = ds.map(parse_data)\n",
        "  print(ds)\n",
        "  #ds = tf.data.FixedLengthRecordDataset(lparse_data(data_files));\n",
        "  #ds = tf.data.Dataset.from_tensor_slices(data_files).map(lambda x : parse_data(x))\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47vs_zxwbzgS",
        "colab_type": "text"
      },
      "source": [
        "##Problem 2. Define FC layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAwLxH90Ormy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fully_connected(batch_size, x, out_dim):\n",
        "    \"\"\"FullyConnected layer for final output.\"\"\"\n",
        "    x = tf.reshape(x, [batch_size, -1])\n",
        "    w = tf.get_variable(\n",
        "        'DW', [x.get_shape()[1], out_dim],\n",
        "        initializer=tf.uniform_unit_scaling_initializer(factor=1.0))\n",
        "    \n",
        "    ################################################\n",
        "    #### FIXME: Create an variable 'b'          ####\n",
        "    #### HINT: name: 'biases', shape: [out_dim] ####\n",
        "    ####       use constant_initializer         ####\n",
        "    ################################################\n",
        "    b = tf.get_variable(\n",
        "      'biases', shape=[out_dim], initializer= tf.constant_initializer(0.1))\n",
        "    \n",
        "    ##################################################\n",
        "    #### FIXME: Create an xw_plus_b op            ####\n",
        "    #### HINT: xw+b (xw is matrix multiplication) ####\n",
        "    ##################################################\n",
        "  \n",
        "    xw_plus_b = tf.malmul(x,w)+b\n",
        "    return xw_plus_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7bOoR5QUTkE",
        "colab_type": "text"
      },
      "source": [
        "## Problem 3. Define optimizer and update operation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN9vleA0UXow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_train_op(loss, lrn_rate):\n",
        "    \"\"\"Build training specific ops for the graph.\"\"\"\n",
        "\n",
        "    #########################################################################\n",
        "    #### FIXME: Create an optimizer using self.lrn_rate as learning rate ####\n",
        "    #########################################################################\n",
        "    update_op = tf.train.GradientDescentOptimizer(learning_rate=self.lrn_rate).minimize(loss)\n",
        "    \n",
        "    return update_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1UUs2SrWwye",
        "colab_type": "text"
      },
      "source": [
        "## Define ResNet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL5o4WI7b2-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"ResNet model.\n",
        "\n",
        "Related papers:\n",
        "https://arxiv.org/pdf/1603.05027v2.pdf\n",
        "https://arxiv.org/pdf/1512.03385v1.pdf\n",
        "https://arxiv.org/pdf/1605.07146v1.pdf\n",
        "\"\"\"\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import six\n",
        "\n",
        "from tensorflow.python.training import moving_averages\n",
        "\n",
        "\n",
        "HParams = namedtuple('HParams',\n",
        "                     'batch_size, num_classes, min_lrn_rate, lrn_rate, '\n",
        "                     'num_residual_units, use_bottleneck, weight_decay_rate, '\n",
        "                     'relu_leakiness')\n",
        "\n",
        "\n",
        "class ResNet(object):\n",
        "  \"\"\"ResNet model.\"\"\"\n",
        "\n",
        "  def __init__(self, hps, images, labels, mode):\n",
        "    \"\"\"ResNet constructor.\n",
        "\n",
        "    Args:\n",
        "      hps: Hyperparameters.\n",
        "      images: Batches of images. [batch_size, image_size, image_size, 3]\n",
        "      labels: Batches of labels. [batch_size, num_classes]\n",
        "      mode: One of 'train' and 'eval'.\n",
        "    \"\"\"\n",
        "    self.hps = hps\n",
        "    self._images = images\n",
        "    self.labels = labels\n",
        "    self.mode = mode\n",
        "\n",
        "    self._extra_train_ops = []\n",
        "\n",
        "  def build_graph(self):\n",
        "    \"\"\"Build a whole graph for the model.\"\"\"\n",
        "    self.global_step = tf.train.get_or_create_global_step()\n",
        "    self._build_model()\n",
        "    if self.mode == 'train':\n",
        "      self.lrn_rate =  tf.constant(self.hps.lrn_rate, tf.float32)\n",
        "      update_op = build_train_op(self.cost, self.lrn_rate)\n",
        "      \n",
        "      with tf.control_dependencies([update_op]):\n",
        "          apply_op = tf.assign_add(self.global_step, 1)\n",
        "   \n",
        "      train_ops = [apply_op] + self._extra_train_ops\n",
        "      self.train_op = tf.group(*train_ops)\n",
        "      \n",
        "    self.summaries = tf.summary.merge_all()\n",
        "\n",
        "  def _stride_arr(self, stride):\n",
        "    \"\"\"Map a stride scalar to the stride array for tf.nn.conv2d.\"\"\"\n",
        "    return [1, stride, stride, 1]\n",
        "\n",
        "  def _build_model(self):\n",
        "    \"\"\"Build the core model within the graph.\"\"\"\n",
        "    with tf.variable_scope('init'):\n",
        "      x = self._images\n",
        "      x = self._conv('init_conv', x, 3, 3, 16, self._stride_arr(1))\n",
        "\n",
        "    strides = [1, 2, 2]\n",
        "    activate_before_residual = [True, False, False]\n",
        "    if self.hps.use_bottleneck:\n",
        "      res_func = self._bottleneck_residual\n",
        "      filters = [16, 64, 128, 256]\n",
        "    else:\n",
        "      res_func = self._residual\n",
        "      filters = [16, 16, 32, 64]\n",
        "      # Uncomment the following codes to use w28-10 wide residual network.\n",
        "      # It is more memory efficient than very deep residual network and has\n",
        "      # comparably good performance.\n",
        "      # https://arxiv.org/pdf/1605.07146v1.pdf\n",
        "      # filters = [16, 160, 320, 640]\n",
        "      # Update hps.num_residual_units to 4\n",
        "\n",
        "    with tf.variable_scope('unit_1_0'):\n",
        "      x = res_func(x, filters[0], filters[1], self._stride_arr(strides[0]),\n",
        "                   activate_before_residual[0])\n",
        "    for i in six.moves.range(1, self.hps.num_residual_units):\n",
        "      with tf.variable_scope('unit_1_%d' % i):\n",
        "        x = res_func(x, filters[1], filters[1], self._stride_arr(1), False)\n",
        "\n",
        "    with tf.variable_scope('unit_2_0'):\n",
        "      x = res_func(x, filters[1], filters[2], self._stride_arr(strides[1]),\n",
        "                   activate_before_residual[1])\n",
        "    for i in six.moves.range(1, self.hps.num_residual_units):\n",
        "      with tf.variable_scope('unit_2_%d' % i):\n",
        "        x = res_func(x, filters[2], filters[2], self._stride_arr(1), False)\n",
        "\n",
        "    with tf.variable_scope('unit_3_0'):\n",
        "      x = res_func(x, filters[2], filters[3], self._stride_arr(strides[2]),\n",
        "                   activate_before_residual[2])\n",
        "    for i in six.moves.range(1, self.hps.num_residual_units):\n",
        "      with tf.variable_scope('unit_3_%d' % i):\n",
        "        x = res_func(x, filters[3], filters[3], self._stride_arr(1), False)\n",
        "\n",
        "    with tf.variable_scope('unit_last'):\n",
        "      x = self._batch_norm('final_bn', x)\n",
        "      x = self._relu(x, self.hps.relu_leakiness)\n",
        "      x = self._global_avg_pool(x)\n",
        "\n",
        "    with tf.variable_scope('logit'):\n",
        "      logits = fully_connected(self.hps.batch_size, x, self.hps.num_classes)\n",
        "      self.predictions = tf.nn.softmax(logits)\n",
        "\n",
        "    with tf.variable_scope('costs'):\n",
        "      xent = tf.nn.softmax_cross_entropy_with_logits(\n",
        "          logits=logits, labels=self.labels)\n",
        "      self.cost = tf.reduce_mean(xent, name='xent')\n",
        "      self.cost += self._decay()\n",
        "\n",
        "      tf.summary.scalar('cost', self.cost)\n",
        "\n",
        "  # TODO(xpan): Consider batch_norm in contrib/layers/python/layers/layers.py\n",
        "  def _batch_norm(self, name, x):\n",
        "    \"\"\"Batch normalization.\"\"\"\n",
        "    with tf.variable_scope(name):\n",
        "      params_shape = [x.get_shape()[-1]]\n",
        "\n",
        "      beta = tf.get_variable(\n",
        "          'beta', params_shape, tf.float32,\n",
        "          initializer=tf.constant_initializer(0.0, tf.float32))\n",
        "      gamma = tf.get_variable(\n",
        "          'gamma', params_shape, tf.float32,\n",
        "          initializer=tf.constant_initializer(1.0, tf.float32))\n",
        "\n",
        "      if self.mode == 'train':\n",
        "        mean, variance = tf.nn.moments(x, [0, 1, 2], name='moments')\n",
        "\n",
        "        moving_mean = tf.get_variable(\n",
        "            'moving_mean', params_shape, tf.float32,\n",
        "            initializer=tf.constant_initializer(0.0, tf.float32),\n",
        "            trainable=False)\n",
        "        moving_variance = tf.get_variable(\n",
        "            'moving_variance', params_shape, tf.float32,\n",
        "            initializer=tf.constant_initializer(1.0, tf.float32),\n",
        "            trainable=False)\n",
        "\n",
        "        self._extra_train_ops.append(moving_averages.assign_moving_average(\n",
        "            moving_mean, mean, 0.9))\n",
        "        self._extra_train_ops.append(moving_averages.assign_moving_average(\n",
        "            moving_variance, variance, 0.9))\n",
        "      else:\n",
        "        mean = tf.get_variable(\n",
        "            'moving_mean', params_shape, tf.float32,\n",
        "            initializer=tf.constant_initializer(0.0, tf.float32),\n",
        "            trainable=False)\n",
        "        variance = tf.get_variable(\n",
        "            'moving_variance', params_shape, tf.float32,\n",
        "            initializer=tf.constant_initializer(1.0, tf.float32),\n",
        "            trainable=False)\n",
        "        tf.summary.histogram(mean.op.name, mean)\n",
        "        tf.summary.histogram(variance.op.name, variance)\n",
        "      # epsilon used to be 1e-5. Maybe 0.001 solves NaN problem in deeper net.\n",
        "      y = tf.nn.batch_normalization(\n",
        "          x, mean, variance, beta, gamma, 0.001)\n",
        "      y.set_shape(x.get_shape())\n",
        "      return y\n",
        "\n",
        "  def _residual(self, x, in_filter, out_filter, stride,\n",
        "                activate_before_residual=False):\n",
        "    \"\"\"Residual unit with 2 sub layers.\"\"\"\n",
        "    if activate_before_residual:\n",
        "      with tf.variable_scope('shared_activation'):\n",
        "        x = self._batch_norm('init_bn', x)\n",
        "        x = self._relu(x, self.hps.relu_leakiness)\n",
        "        orig_x = x\n",
        "    else:\n",
        "      with tf.variable_scope('residual_only_activation'):\n",
        "        orig_x = x\n",
        "        x = self._batch_norm('init_bn', x)\n",
        "        x = self._relu(x, self.hps.relu_leakiness)\n",
        "\n",
        "    with tf.variable_scope('sub1'):\n",
        "      x = self._conv('conv1', x, 3, in_filter, out_filter, stride)\n",
        "\n",
        "    with tf.variable_scope('sub2'):\n",
        "      x = self._batch_norm('bn2', x)\n",
        "      x = self._relu(x, self.hps.relu_leakiness)\n",
        "      x = self._conv('conv2', x, 3, out_filter, out_filter, [1, 1, 1, 1])\n",
        "\n",
        "    with tf.variable_scope('sub_add'):\n",
        "      if in_filter != out_filter:\n",
        "        orig_x = tf.nn.avg_pool(orig_x, stride, stride, 'VALID')\n",
        "        orig_x = tf.pad(\n",
        "            orig_x, [[0, 0], [0, 0], [0, 0],\n",
        "                     [(out_filter-in_filter)//2, (out_filter-in_filter)//2]])\n",
        "      x += orig_x\n",
        "\n",
        "    tf.logging.debug('image after unit %s', x.get_shape())\n",
        "    return x\n",
        "\n",
        "  def _bottleneck_residual(self, x, in_filter, out_filter, stride,\n",
        "                           activate_before_residual=False):\n",
        "    \"\"\"Bottleneck residual unit with 3 sub layers.\"\"\"\n",
        "    if activate_before_residual:\n",
        "      with tf.variable_scope('common_bn_relu'):\n",
        "        x = self._batch_norm('init_bn', x)\n",
        "        x = self._relu(x, self.hps.relu_leakiness)\n",
        "        orig_x = x\n",
        "    else:\n",
        "      with tf.variable_scope('residual_bn_relu'):\n",
        "        orig_x = x\n",
        "        x = self._batch_norm('init_bn', x)\n",
        "        x = self._relu(x, self.hps.relu_leakiness)\n",
        "\n",
        "    with tf.variable_scope('sub1'):\n",
        "      x = self._conv('conv1', x, 1, in_filter, out_filter/4, stride)\n",
        "\n",
        "    with tf.variable_scope('sub2'):\n",
        "      x = self._batch_norm('bn2', x)\n",
        "      x = self._relu(x, self.hps.relu_leakiness)\n",
        "      x = self._conv('conv2', x, 3, out_filter/4, out_filter/4, [1, 1, 1, 1])\n",
        "\n",
        "    with tf.variable_scope('sub3'):\n",
        "      x = self._batch_norm('bn3', x)\n",
        "      x = self._relu(x, self.hps.relu_leakiness)\n",
        "      x = self._conv('conv3', x, 1, out_filter/4, out_filter, [1, 1, 1, 1])\n",
        "\n",
        "    with tf.variable_scope('sub_add'):\n",
        "      if in_filter != out_filter:\n",
        "        orig_x = self._conv('project', orig_x, 1, in_filter, out_filter, stride)\n",
        "      x += orig_x\n",
        "\n",
        "    tf.logging.info('image after unit %s', x.get_shape())\n",
        "    return x\n",
        "\n",
        "  def _decay(self):\n",
        "    \"\"\"L2 weight decay loss.\"\"\"\n",
        "    costs = []\n",
        "    for var in tf.trainable_variables():\n",
        "      if var.op.name.find(r'DW') > 0:\n",
        "        costs.append(tf.nn.l2_loss(var))\n",
        "        # tf.summary.histogram(var.op.name, var)\n",
        "\n",
        "    return tf.multiply(self.hps.weight_decay_rate, tf.add_n(costs))\n",
        "\n",
        "  def _conv(self, name, x, filter_size, in_filters, out_filters, strides):\n",
        "    \"\"\"Convolution.\"\"\"\n",
        "    with tf.variable_scope(name):\n",
        "      n = filter_size * filter_size * out_filters\n",
        "      kernel = tf.get_variable(\n",
        "          'DW', [filter_size, filter_size, in_filters, out_filters],\n",
        "          tf.float32, initializer=tf.random_normal_initializer(\n",
        "              stddev=np.sqrt(2.0/n)))\n",
        "      return tf.nn.conv2d(x, kernel, strides, padding='SAME')\n",
        "\n",
        "  def _relu(self, x, leakiness=0.0):\n",
        "    \"\"\"Relu, with optional leaky support.\"\"\"\n",
        "    return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')\n",
        "\n",
        "  def _global_avg_pool(self, x):\n",
        "    assert x.get_shape().ndims == 4\n",
        "    return tf.reduce_mean(x, [1, 2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYszRyxjb2aZ",
        "colab_type": "text"
      },
      "source": [
        "## Train ResNet model\n",
        "Do not be frightened if you face such an error: \n",
        "`An exception has occurred, use %tb to see the full traceback.  SystemExit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR7jlpOUdQhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"ResNet Train/Eval module.\n",
        "\"\"\"\n",
        "import time\n",
        "import six\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Global\n",
        "train_data_path = './cifar-10-batches-bin/data_batch*'\n",
        "image_size = 32\n",
        "ckpt_dir = './train_ckpt'\n",
        "ckpt_prefix = ckpt_dir + '/cifar10-train'\n",
        "\n",
        "def train(hps):\n",
        "  \"\"\"Training loop.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    images, labels = build_input(\n",
        "    'cifar10', train_data_path, hps.batch_size, 'train')\n",
        "    model = ResNet(hps, images, labels, 'train')\n",
        "    model.build_graph()\n",
        "\n",
        "    truth = tf.argmax(model.labels, axis=1)\n",
        "    predictions = tf.argmax(model.predictions, axis=1)\n",
        "    precision = tf.reduce_mean(tf.to_float(tf.equal(predictions, truth)))\n",
        "  \n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    saver = tf.train.Saver(max_to_keep=10000)\n",
        "  \n",
        "    with tf.Session() as sess:\n",
        "      sess.run(init)\n",
        "      for i in range(3001):\n",
        "        _, global_step, cost, precision_ = \\\n",
        "          sess.run([model.train_op, model.global_step, model.cost, precision])\n",
        "    \n",
        "        if global_step % 100 == 0:\n",
        "          print('step: %d, loss: %.3f, precision: %.3f' % (global_step, cost, precision_))\n",
        "\n",
        "          saver.save(sess, ckpt_prefix, global_step=i)\n",
        "\n",
        "        \n",
        "def main(_):\n",
        "  batch_size = 128\n",
        "\n",
        "  hps = HParams(batch_size=batch_size,\n",
        "                             num_classes=10,\n",
        "                             min_lrn_rate=0.0001,\n",
        "                             lrn_rate=0.1,\n",
        "                             num_residual_units=5,\n",
        "                             use_bottleneck=False,\n",
        "                             weight_decay_rate=0.0002,\n",
        "                             relu_leakiness=0.1)\n",
        "\n",
        "  train(hps)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y3HiS9MtcxW",
        "colab_type": "text"
      },
      "source": [
        "### You can see *cifar10-train-0~3000* checkpoint files when you run following code, after you train model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg6E42w4oTB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls train_ckpt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBDxI6zhoyY_",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate trained ResNet model\n",
        "\n",
        "Before you run this code\n",
        "click Runtime->**restart runtime**\n",
        "\n",
        "(If you want to erase all the local files, then click *RESET ALL RUNTIMES* or **DO NOT CLICK!**)\n",
        "\n",
        "and restart **Define the Resnet50 Model**,  **CIFAR10 input**\n",
        "\n",
        "Do not be frightened if you face such an error: \n",
        "`An exception has occurred, use %tb to see the full traceback.  SystemExit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj2qiublo3hU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf './tensorboard'\n",
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"ResNet Train/Eval module.\n",
        "\"\"\"\n",
        "import time\n",
        "import six\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "eval_data_path = './cifar-10-batches-bin/test_batch.bin'\n",
        "ckpt_dir = './train_ckpt'\n",
        "tensorboard_path = './tensorboard'\n",
        "\n",
        "def evaluate(hps):\n",
        "  \"\"\"Eval loop.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    images, labels = build_input(\n",
        "      'cifar10', './cifar-10-batches-bin/test_batch.bin', hps.batch_size, 'eval')\n",
        "    model = ResNet(hps, images, labels, 'eval')\n",
        "    model.build_graph()\n",
        "\n",
        "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
        "  \n",
        "    saver = tf.train.Saver() \n",
        "    \n",
        "    summary_writer = tf.summary.FileWriter('./tensorboard', sess.graph)\n",
        "\n",
        "    try:\n",
        "      ckpt_state = tf.train.get_checkpoint_state(ckpt_dir)\n",
        "    except tf.errors.OutOfRangeError as e:\n",
        "      tf.logging.error('Cannot restore checkpoint: %s', e)\n",
        "    if not (ckpt_state):\n",
        "      tf.logging.info('No model to eval yet at %s', ckpt_dir)\n",
        "    \n",
        "    best_precision = 0.\n",
        "    for i in range(len(ckpt_state.all_model_checkpoint_paths)):\n",
        "      tf.logging.info('Loading checkpoint %s', ckpt_state.all_model_checkpoint_paths[i])\n",
        "      saver.restore(sess, ckpt_state.all_model_checkpoint_paths[i])\n",
        "      total_prediction, correct_prediction = 0, 0\n",
        "\n",
        "      for _ in six.moves.range(100):\n",
        "        (summaries, loss, predictions, truth, train_step) = sess.run(\n",
        "          [model.summaries, model.cost, model.predictions,\n",
        "           model.labels, model.global_step])\n",
        "\n",
        "        truth = np.argmax(truth, axis=1)\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        correct_prediction += np.sum(truth == predictions)\n",
        "        total_prediction += predictions.shape[0]\n",
        "\n",
        "      precision = 1.0 * correct_prediction / total_prediction\n",
        "      best_precision = max(precision, best_precision)\n",
        "    \n",
        "      precision_summ = tf.Summary()\n",
        "      precision_summ.value.add(\n",
        "        tag='Precision', simple_value=precision)\n",
        "      summary_writer.add_summary(precision_summ, train_step)\n",
        "      best_precision_summ = tf.Summary()\n",
        "      best_precision_summ.value.add(\n",
        "        tag='Best Precision', simple_value=best_precision)\n",
        "      summary_writer.add_summary(best_precision_summ, train_step)\n",
        "      summary_writer.add_summary(summaries, train_step)\n",
        "     \n",
        "      tf.logging.info('loss: %.3f, precision: %.3f, best precision: %.3f' %\n",
        "                      (loss, precision, best_precision))\n",
        "      summary_writer.flush()\n",
        "\n",
        "      tf.logging.info('step: %d, loss: %.3f, precision: %.3f' %\n",
        "                      (i * 100, loss, precision))\n",
        "\n",
        "\n",
        "def main(_):\n",
        "\n",
        "  hps = HParams(batch_size=100,\n",
        "                num_classes=10,\n",
        "                min_lrn_rate=0.0001,\n",
        "                lrn_rate=0.1,\n",
        "                num_residual_units=5,\n",
        "                use_bottleneck=False,\n",
        "                weight_decay_rate=0.0002,\n",
        "                relu_leakiness=0.1)\n",
        "\n",
        "  evaluate(hps)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlnAphXnUH_M",
        "colab_type": "text"
      },
      "source": [
        "### Display our graph on tensorboard!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LodXGDkTuIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "#run tensorboard\n",
        "LOG_DIR = './tensorboard'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "#run ngrok\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGvkGP2AT97B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}